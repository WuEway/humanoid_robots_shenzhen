import numpy as np
import cv2
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import time

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField
from geometry_msgs.msg import PoseStamped, TransformStamped, PointStamped
from std_msgs.msg import Header
from cv_bridge import CvBridge
from image_to_grasp.srv import ImageToGrasp
import tf2_ros
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
import tf2_geometry_msgs
from tf2_ros import LookupException, ConnectivityException, ExtrapolationException
# from .grounding_dino_processor import AdvancedGroundingDinoProcessor
# # å¯¼å…¥åŸæœ‰å¤„ç†å™¨å’ŒæŠ“å–ä½å§¿ä¼°è®¡å™¨
from .grasp_pose_estimator import GraspPoseEstimator 


class AdvancedGroundingDinoProcessor:
    """ä¿æŒåŸæœ‰å¤„ç†å™¨é€»è¾‘ä¸å˜ï¼Œè´Ÿè´£æ¨¡å‹åŠ è½½ã€æ£€æµ‹å’Œç‚¹äº‘æå–"""
    def __init__(self, 
                 grounding_dino_config_path: str = "non_ros_pkg/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py",
                 grounding_dino_checkpoint_path: str = "non_ros_pkg/Grounded-Segment-Anything/groundingdino_swint_ogc.pth",
                 sam_encoder_version: str = "vit_h",
                 sam_checkpoint_path: str = "non_ros_pkg/Grounded-Segment-Anything/sam_vit_h_4b8939.pth",
                 box_threshold: float = 0.35,
                 text_threshold: float = 0.25,
                 nms_threshold: float = 0.5,
                 device: str = "cuda"):
        
        self.box_threshold = box_threshold
        self.text_threshold = text_threshold
        self.nms_threshold = nms_threshold
        
        import torch
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # æ¨¡å‹è·¯å¾„å¤„ç†
        script_dir = os.path.dirname(os.path.abspath(__file__))
        workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
        self.grounding_dino_config_path = os.path.join(workspace_root, grounding_dino_config_path)
        self.grounding_dino_checkpoint_path = os.path.join(workspace_root, grounding_dino_checkpoint_path)
        self.sam_encoder_version = sam_encoder_version
        self.sam_checkpoint_path = os.path.join(workspace_root, sam_checkpoint_path)

        # æ¨¡å‹åˆå§‹åŒ–
        self.grounding_dino_model = None
        self.sam_predictor = None
        self._load_models()
        
        print(f"GroundingDinoå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _load_models(self):
        from groundingdino.util.inference import Model
        from segment_anything import sam_model_registry, SamPredictor
        
        print("æ­£åœ¨åŠ è½½GroundingDinoæ¨¡å‹...")
        self.grounding_dino_model = Model(
            model_config_path=self.grounding_dino_config_path,
            model_checkpoint_path=self.grounding_dino_checkpoint_path
        )
        print("GroundingDinoæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        print("æ­£åœ¨åŠ è½½SAMæ¨¡å‹...")
        sam = sam_model_registry[self.sam_encoder_version](checkpoint=self.sam_checkpoint_path)
        sam.to(device=self.device)
        self.sam_predictor = SamPredictor(sam)
        print("SAMæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    def process(self, color_image: np.ndarray, depth_image: np.ndarray, text_prompt: str = "object", camera_intrinsics: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        if color_image is None or depth_image is None:
            return {"success": False, "error": "Invalid image data"}
            
        try:
            # 1. ç›®æ ‡æ£€æµ‹
            detections = self._detect_objects(color_image, text_prompt)
            
            # 2. æå–ç‚¹äº‘
            point_clouds = []
            for detection in detections:
                point_cloud_data = self._extract_point_cloud(color_image, depth_image, detection, camera_intrinsics)
                if point_cloud_data is not None:
                    point_clouds.append({
                        "detection": detection,
                        "point_cloud": point_cloud_data
                    })
            
            # 3. å¯è§†åŒ–ç»“æœ
            result_image = self._visualize_detections(color_image, detections)

            return {
                "success": True,
                "detections": detections,
                "point_clouds": point_clouds,
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _detect_objects(self, image: np.ndarray, text_prompt: str) -> List[Dict]:
        detections = []
        try:
            # å¤„ç†æ–‡æœ¬æç¤º
            classes = [c.strip() for c in text_prompt.split(".") if c.strip()] if isinstance(text_prompt, str) else text_prompt
            if not classes:
                return []
            
            # GroundingDinoæ£€æµ‹
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            detections_sv = self.grounding_dino_model.predict_with_classes(
                image=rgb_image,
                classes=classes,
                box_threshold=self.box_threshold,
                text_threshold=self.text_threshold
            )
            
            if len(detections_sv.xyxy) == 0:
                return []
            
            # NMSåå¤„ç†
            import torch
            import torchvision
            nms_idx = torchvision.ops.nms(
                torch.from_numpy(detections_sv.xyxy),
                torch.from_numpy(detections_sv.confidence),
                self.nms_threshold
            ).numpy().tolist()
            
            filtered_boxes = detections_sv.xyxy[nms_idx]
            filtered_confidences = detections_sv.confidence[nms_idx]
            filtered_class_ids = detections_sv.class_id[nms_idx]
            
            # SAMåˆ†å‰²
            masks = self._segment_with_sam(rgb_image, filtered_boxes)
            
            # æ ¼å¼åŒ–ç»“æœ
            all_detections = []
            for i, (box, confidence, class_id, mask) in enumerate(
                zip(filtered_boxes, filtered_confidences, filtered_class_ids, masks)
            ):
                x1, y1, x2, y2 = box.astype(int)
                all_detections.append({
                    "bbox": [x1, y1, x2 - x1, y2 - y1],
                    "xyxy": [x1, y1, x2, y2],
                    "confidence": float(confidence),
                    "class_id": int(class_id),
                    "label": classes[class_id] if class_id < len(classes) else "object",
                    "mask": mask
                })
            
            # ä¿ç•™æ¯ç±»æœ€é«˜ç½®ä¿¡åº¦ç»“æœ
            detections = self._pick_best_detection_per_class(all_detections)
            return detections
            
        except Exception as e:
            print(f"æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            return []
    
    def _pick_best_detection_per_class(self, detections: List[Dict]) -> List[Dict]:
        if not detections:
            return detections
        
        class_groups = {}
        for detection in detections:
            label = detection["label"]
            class_groups[label] = class_groups.get(label, []) + [detection]
        
        filtered_detections = []
        for label, group in class_groups.items():
            best_detection = max(group, key=lambda x: x["confidence"])
            filtered_detections.append(best_detection)
        
        return filtered_detections

    def _segment_with_sam(self, image: np.ndarray, boxes: np.ndarray) -> List[np.ndarray]:
        self.sam_predictor.set_image(image)
        result_masks = []
        for box in boxes:
            masks, scores, logits = self.sam_predictor.predict(box=box, multimask_output=True)
            result_masks.append(masks[np.argmax(scores)])
        return result_masks
    
    def _extract_point_cloud(self, color_image: np.ndarray, depth_image: np.ndarray, detection: Dict, camera_intrinsics: Optional[Dict[str, float]] = None) -> Optional[Dict]:
        try:
            mask = detection.get("mask")
            if mask is None:
                return None
            
            h, w = color_image.shape[:2]
            # ç›¸æœºå†…å‚å¤„ç†
            if camera_intrinsics:
                fx, fy = camera_intrinsics["fx"], camera_intrinsics["fy"]
                cx, cy = camera_intrinsics["cx"], camera_intrinsics["cy"]
            else:
                fx, fy = 525.0, 525.0
                cx, cy = w / 2.0, h / 2.0
            
            # æå–æ©ç åŒºåŸŸåæ ‡
            y_coords, x_coords = np.where(mask > 0)
            points = []
            colors = []
            
            for y, x in zip(y_coords, x_coords):
                depth = depth_image[y, x]
                if depth <= 0:
                    continue
                # åƒç´ â†’ç›¸æœº3Dåæ ‡è½¬æ¢
                z = depth / 1000.0  # æ·±åº¦å›¾å•ä½å‡è®¾ä¸ºmm
                x_3d = (x - cx) * z / fx
                y_3d = (y - cy) * z / fy
                
                points.append([x_3d, y_3d, z])
                # é¢œè‰²è½¬æ¢ï¼ˆBGRâ†’RGBï¼‰
                b, g, r = color_image[y, x]
                colors.append([r, g, b])
            
            if len(points) == 0:
                return None
            
            return {
                "points": np.array(points, dtype=np.float32),
                "colors": np.array(colors, dtype=np.uint8)
            }
            
        except Exception as e:
            print(f"ç‚¹äº‘æå–å¤±è´¥: {e}")
            return None
    
    def _visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        result_image = image.copy()
        colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
        
        for i, detection in enumerate(detections):
            bbox = detection["bbox"]
            confidence = detection["confidence"]
            label = detection["label"]
            x, y, w, h = bbox
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            # ç»˜åˆ¶æ ‡ç­¾
            text = f"{label}: {confidence:.2f}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(result_image, (x, y - text_size[1] - 5), (x + text_size[0], y), color, -1)
            cv2.putText(result_image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            # ç»˜åˆ¶æ©ç 
            if "mask" in detection and detection["mask"] is not None:
                mask = detection["mask"].astype(np.uint8) * 255
                colored_mask = np.zeros_like(result_image)
                colored_mask[mask > 0] = color
                result_image = cv2.addWeighted(result_image, 0.7, colored_mask, 0.3, 0)
        
        return result_image


class GroundingDinoGraspServer(Node):
    """ROS 2 æœåŠ¡ç«¯:æ¥æ”¶å›¾åƒè¯·æ±‚, è¿”å›æŠ“å–ä½å§¿å¹¶å‘å¸ƒTF"""
    def __init__(self):
        super().__init__("grounding_dino_grasp_server")
        
        # 1. å£°æ˜å‚æ•°ï¼ˆå¯é€šè¿‡å¯åŠ¨æ–‡ä»¶æˆ–å‘½ä»¤è¡Œä¿®æ”¹ï¼‰
        self.declare_parameter("grasp_food_pos_frame", "grasp_food_pos")
        self.declare_parameter("detection_prompt", "delivery box. pink takeout bag")
        self.declare_parameter("confidence_threshold", 0.4)
        self.declare_parameter("target_id_in_prompt", 1)  # 1å¯¹åº”"pink takeout bag"
        self.declare_parameter("robot_base_frame", "woosh_base_link")
        # self.declare_parameter("camera_frame", "woosh_head_rgbd_color_optical_frame")
        self.declare_parameter("camera_frame", "woosh_left_hand_rgbd_color_optical_frame")
        
        # è·å–å‚æ•°
        self.grasp_frame = self.get_parameter("grasp_food_pos_frame").get_parameter_value().string_value
        self.detect_prompt = self.get_parameter("detection_prompt").get_parameter_value().string_value
        self.conf_thresh = self.get_parameter("confidence_threshold").get_parameter_value().double_value
        self.target_id = self.get_parameter("target_id_in_prompt").get_parameter_value().integer_value
        self.base_frame = self.get_parameter("robot_base_frame").get_parameter_value().string_value
        self.camera_frame = self.get_parameter("camera_frame").get_parameter_value().string_value
        
        # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.bridge = CvBridge()
        # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆåŠ è½½GroundingDino+SAMæ¨¡å‹ï¼‰
        self.processor = AdvancedGroundingDinoProcessor()
        # åˆå§‹åŒ–æŠ“å–ä½å§¿ä¼°è®¡å™¨
        self.grasp_estimator = GraspPoseEstimator(visualize=False)
        # åˆå§‹åŒ–TFï¼ˆç”¨äºåæ ‡è½¬æ¢å’Œå‘å¸ƒï¼‰
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)
        # åˆå§‹åŒ–æŠ“å–ä½å§¿å‘å¸ƒè€…ï¼ˆå¯é€‰ï¼Œç”¨äºè¯é¢˜å‘å¸ƒï¼‰
        self.grasp_pub = self.create_publisher(PoseStamped, "/grounding_dino/grasp_pose", 10)
        
        # 3. è§£æç›®æ ‡æ ‡ç­¾ï¼ˆä»æ£€æµ‹æç¤ºä¸­æå–ï¼‰
        self.target_label = None
        prompt_classes = [c.strip() for c in self.detect_prompt.split(".") if c.strip()]
        if 0 <= self.target_id < len(prompt_classes):
            self.target_label = prompt_classes[self.target_id]
            self.get_logger().info(f"ç›®æ ‡æŠ“å–æ ‡ç­¾: '{self.target_label}'")
        else:
            self.get_logger().error(f"æ— æ•ˆtarget_id_in_prompt: {self.target_id}ï¼ŒæœåŠ¡å°†æ— æ³•æ­£å¸¸å·¥ä½œ")
            return
        
        # 4. ç›¸æœºå†…å‚ï¼ˆæ ¹æ®å®é™…ç›¸æœºè°ƒæ•´ï¼‰
        self.cam_intrinsics = {
            "fx": 427.8312,
            "fy": 427.3405,
            "cx": 430.8444,
            "cy": 246.7171
        }
        self.get_logger().info(f"ç›¸æœºå†…å‚: fx={self.cam_intrinsics['fx']}, fy={self.cam_intrinsics['fy']}, cx={self.cam_intrinsics['cx']}, cy={self.cam_intrinsics['cy']}")
        
        # 5. åˆ›å»ºæœåŠ¡ï¼ˆæœåŠ¡ç±»å‹ï¼šImageToPoseï¼‰
        self.grasp_service = self.create_service(
            ImageToGrasp,  # æ›¿æ¢ä¸ºä½ çš„æœåŠ¡æ¶ˆæ¯ç±»å‹
            "/grounding_dino/image_to_grasp",  # æœåŠ¡è¯é¢˜
            self.handle_grasp_request  # æœåŠ¡å›è°ƒå‡½æ•°
        )
        
        self.get_logger().info("âœ… GroundingDinoæŠ“å–æœåŠ¡ç«¯å¯åŠ¨å®Œæˆ")
        self.get_logger().info(f"æœåŠ¡è¯é¢˜: /grounding_dino/image_to_grasp")
        self.get_logger().info(f"TFå‘å¸ƒ: {self.base_frame} â†’ {self.grasp_frame}")

    def handle_grasp_request(self, request, response):
        """æœåŠ¡å›è°ƒå‡½æ•°ï¼šå¤„ç†å®¢æˆ·ç«¯çš„å›¾åƒè¯·æ±‚ï¼Œç”ŸæˆæŠ“å–ä½å§¿"""
        self.get_logger().info("ğŸ“¥ æ”¶åˆ°å®¢æˆ·ç«¯å›¾åƒè¯·æ±‚ï¼Œå¼€å§‹å¤„ç†...")
        
        try:
            # 1. è§£æå®¢æˆ·ç«¯è¯·æ±‚ä¸­çš„å›¾åƒï¼ˆRGB + æ·±åº¦ï¼‰
            # è½¬æ¢RGBå›¾åƒï¼ˆsensor_msgs/Image â†’ OpenCVï¼‰
            color_img = self.bridge.imgmsg_to_cv2(request.color_image, "bgr8")
            # è½¬æ¢æ·±åº¦å›¾åƒï¼ˆå‡è®¾æ·±åº¦å›¾æ ¼å¼ä¸º16UC1ï¼Œå•ä½mmï¼‰
            depth_img = self.bridge.imgmsg_to_cv2(request.depth_image, "16UC1")
            
            # 2. æ‰§è¡Œæ£€æµ‹å’Œç‚¹äº‘æå–ï¼ˆè°ƒç”¨å¤„ç†å™¨ï¼‰
            process_result = self.processor.process(
                color_image=color_img,
                depth_image=depth_img,
                text_prompt=self.detect_prompt,
                camera_intrinsics=self.cam_intrinsics
            )
            
            if not process_result["success"]:
                response.success = False
                response.message = f"æ£€æµ‹å¤±è´¥: {process_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                self.get_logger().error(response.message)
                return response
            
            # 3. ç­›é€‰ç›®æ ‡ç‚¹äº‘ï¼ˆåªä¿ç•™ç›®æ ‡æ ‡ç­¾çš„ç‚¹äº‘ï¼‰
            target_pointcloud = None
            for pc_item in process_result["point_clouds"]:
                det_label = pc_item["detection"]["label"]
                det_conf = pc_item["detection"]["confidence"]
                if det_label == self.target_label and det_conf >= self.conf_thresh:
                    target_pointcloud = pc_item["point_cloud"]
                    break
            
            if target_pointcloud is None:
                response.success = False
                response.message = f"æœªæ£€æµ‹åˆ°ç›®æ ‡æ ‡ç­¾: '{self.target_label}'ï¼ˆæˆ–ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼‰"
                self.get_logger().warn(response.message)
                return response
            
            # 4. åæ ‡è½¬æ¢ï¼šç›¸æœºå¸§ â†’ æœºå™¨äººåŸºåº§å¸§
            points_cam = target_pointcloud["points"]
            points_base = self._transform_point_cloud(points_cam, self.camera_frame, self.base_frame)
            if points_base is None or len(points_base) == 0:
                response.success = False
                response.message = "ç‚¹äº‘åæ ‡è½¬æ¢å¤±è´¥ï¼ˆç›¸æœºâ†’åŸºåº§ï¼‰"
                self.get_logger().error(response.message)
                return response
            
            # 5. è®¡ç®—æŠ“å–ä½å§¿
            grasp_result = self.grasp_estimator.calculate_grasp_pose(points_base, target_pointcloud["colors"])
            if not grasp_result:
                response.success = False
                response.message = "æŠ“å–ä½å§¿è®¡ç®—å¤±è´¥"
                self.get_logger().error(response.message)
                return response
            
            grasp_point, grasp_quat = grasp_result
            
            # 6. å‘å¸ƒæŠ“å–ä½å§¿åˆ°TFå’Œè¯é¢˜
            self._publish_grasp_tf(grasp_point, grasp_quat)
            self._publish_grasp_topic(grasp_point, grasp_quat)
            
            # 7. æ„å»ºæœåŠ¡å“åº”ï¼ˆè¿”å›æŠ“å–ä½å§¿ç»™å®¢æˆ·ç«¯ï¼‰
            response.success = True
            response.message = f"æŠ“å–ä½å§¿ç”ŸæˆæˆåŠŸ, å·²å‘å¸ƒTF: {self.base_frame} â†’ {self.grasp_frame}"
            response.grasp_pose.position = grasp_point
            response.grasp_pose.orientation = grasp_quat
            
            self.get_logger().info(f"âœ… å¤„ç†å®Œæˆï¼{response.message}")
            return response
            
        except Exception as e:
            response.success = False
            response.message = f"æœåŠ¡å¤„ç†å¼‚å¸¸: {str(e)}"
            self.get_logger().error(f"âŒ æœåŠ¡å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            return response

    def _transform_point_cloud(self, points_cam: np.ndarray, source_frame: str, target_frame: str) -> Optional[np.ndarray]:
        """å°†ç›¸æœºå¸§ç‚¹äº‘è½¬æ¢åˆ°æœºå™¨äººåŸºåº§å¸§"""
        if points_cam.size == 0:
            return None
        
        try:
            # è·å–ç›¸æœºâ†’åŸºåº§çš„TFå˜æ¢
            transform = self.tf_buffer.lookup_transform(
                target_frame,
                source_frame,
                rclpy.time.Time()  # è·å–æœ€æ–°å˜æ¢
            )
            
            # é€ç‚¹è½¬æ¢ï¼ˆé€‚åˆä¸­å°è§„æ¨¡ç‚¹äº‘ï¼‰
            transformed_points = []
            for point in points_cam:
                p_stamped = PointStamped()
                p_stamped.header.frame_id = source_frame
                p_stamped.point.x = float(point[0])
                p_stamped.point.y = float(point[1])
                p_stamped.point.z = float(point[2])
                
                # åº”ç”¨TFå˜æ¢
                p_trans = tf2_geometry_msgs.do_transform_point(p_stamped, transform)
                transformed_points.append([p_trans.point.x, p_trans.point.y, p_trans.point.z])
            
            return np.array(transformed_points, dtype=np.float32)
        
        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"TFè½¬æ¢å¤±è´¥: {source_frame} â†’ {target_frame}: {str(e)}")
            return None

    def _publish_grasp_tf(self, grasp_point, grasp_quat):
        """å‘å¸ƒæŠ“å–ä½å§¿åˆ°TF"""
        t = TransformStamped()
        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = self.base_frame
        t.child_frame_id = self.grasp_frame
        # ä½ç½®
        t.transform.translation.x = grasp_point.x
        t.transform.translation.y = grasp_point.y
        t.transform.translation.z = grasp_point.z
        # å§¿æ€ï¼ˆå››å…ƒæ•°ï¼‰
        t.transform.rotation.x = grasp_quat.x
        t.transform.rotation.y = grasp_quat.y
        t.transform.rotation.z = grasp_quat.z
        t.transform.rotation.w = grasp_quat.w
        
        self.tf_broadcaster.sendTransform(t)

    def _publish_grasp_topic(self, grasp_point, grasp_quat):
        """å‘å¸ƒæŠ“å–ä½å§¿åˆ°è¯é¢˜ï¼ˆå¯é€‰ï¼Œä¾›å…¶ä»–èŠ‚ç‚¹è®¢é˜…ï¼‰"""
        pose_msg = PoseStamped()
        pose_msg.header.stamp = self.get_clock().now().to_msg()
        pose_msg.header.frame_id = self.base_frame
        pose_msg.pose.position = grasp_point
        pose_msg.pose.orientation = grasp_quat
        
        self.grasp_pub.publish(pose_msg)


def main():
    rclpy.init()
    # åˆ›å»ºæœåŠ¡ç«¯èŠ‚ç‚¹
    server = GroundingDinoGraspServer()
    try:
        rclpy.spin(server)
    except KeyboardInterrupt:
        server.get_logger().info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼ŒæœåŠ¡ç«¯é€€å‡º")
    finally:
        # æ¸…ç†èµ„æºï¼ˆå…³é—­å¯è§†åŒ–çª—å£ç­‰ï¼‰
        cv2.destroyAllWindows()
        server.destroy_node()
        rclpy.shutdown()


if __name__ == "__main__":
    main()