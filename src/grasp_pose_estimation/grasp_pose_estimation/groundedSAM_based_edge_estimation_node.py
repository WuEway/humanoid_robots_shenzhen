"""
GroundingDinoå¤„ç†å™¨çš„è¯¦ç»†å®ç°
åŒ…å«ç›®æ ‡æ£€æµ‹å’Œç‚¹äº‘æå–åŠŸèƒ½
é›†æˆçœŸå®çš„GroundingDino+SAMæ¨¡å‹
åŸºäºROS2ç‰ˆæœ¬
"""

import numpy as np
import cv2
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import time

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField
from geometry_msgs.msg import PoseStamped, TransformStamped
from std_msgs.msg import Header
from cv_bridge import CvBridge
from .grasp_pose_estimator import GraspPoseEstimator 


import tf2_ros
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
from geometry_msgs.msg import PointStamped
import tf2_geometry_msgs # å¯¼å…¥å˜æ¢å‡½æ•°åº“
from tf2_ros import LookupException, ConnectivityException, ExtrapolationException





class AdvancedGroundingDinoProcessor:
    """é«˜çº§GroundingDinoå¤„ç†å™¨ - é›†æˆçœŸå®çš„GroundingDino+SAMæ¨¡å‹"""
    
    def __init__(self, 
                 grounding_dino_config_path: str = "non_ros_pkg/Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py",
                 grounding_dino_checkpoint_path: str = "non_ros_pkg/Grounded-Segment-Anything/groundingdino_swint_ogc.pth",
                 sam_encoder_version: str = "vit_h",
                 sam_checkpoint_path: str = "non_ros_pkg/Grounded-Segment-Anything/sam_vit_h_4b8939.pth",
                 box_threshold: float = 0.35,
                 text_threshold: float = 0.25,
                 nms_threshold: float = 0.5,
                 device: str = "cuda"):
        
        self.box_threshold = box_threshold
        self.text_threshold = text_threshold
        self.nms_threshold = nms_threshold
        
        # åˆå§‹åŒ–torchè®¾å¤‡
        import torch
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        import os
        from pathlib import Path

        
        # æ¨¡å‹è·¯å¾„
        # è·å–å½“å‰è„šæœ¬æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
        self.grounding_dino_config_path = os.path.join(workspace_root, grounding_dino_config_path)
        self.grounding_dino_checkpoint_path = os.path.join(workspace_root, grounding_dino_checkpoint_path)
        self.sam_encoder_version = sam_encoder_version
        self.sam_checkpoint_path = os.path.join(workspace_root, sam_checkpoint_path)

        # åˆå§‹åŒ–æ¨¡å‹
        self.grounding_dino_model = None
        self.sam_predictor = None
        
        # å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
        self.last_detection_time = None
        self.detection_interval = 0.0

        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        self._load_models()
        
        print(f"GroundingDinoå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _load_models(self):
        """åŠ è½½GroundingDinoå’ŒSAMæ¨¡å‹"""
        # å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
        from groundingdino.util.inference import Model
        from segment_anything import sam_model_registry, SamPredictor
        
        # åŠ è½½GroundingDinoæ¨¡å‹
        print("æ­£åœ¨åŠ è½½GroundingDinoæ¨¡å‹...")
        self.grounding_dino_model = Model(
            model_config_path=self.grounding_dino_config_path,
            model_checkpoint_path=self.grounding_dino_checkpoint_path
        )
        print("GroundingDinoæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½SAMæ¨¡å‹
        print("æ­£åœ¨åŠ è½½SAMæ¨¡å‹...")
        sam = sam_model_registry[self.sam_encoder_version](checkpoint=self.sam_checkpoint_path)
        sam.to(device=self.device)
        self.sam_predictor = SamPredictor(sam)
        print("SAMæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    def process(self, color_image: np.ndarray, depth_image: np.ndarray, text_prompt: str = "object", camera_intrinsics: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """
        å¤„ç†RGBDæ•°æ®, æ£€æµ‹ç›®æ ‡å¹¶æå–ç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            text_prompt: æ£€æµ‹ç›®æ ‡çš„æ–‡æœ¬æè¿°
            
        Returns:
            åŒ…å«æ£€æµ‹ç»“æœå’Œç‚¹äº‘çš„å­—å…¸
        """
        if color_image is None or depth_image is None:
            return {"success": False, "error": "Invalid image data"}
            
        try:
            # æ›´æ–°å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
            current_time = time.time()
            
            # è®¡ç®—æ£€æµ‹é—´éš”æ—¶é—´
            if self.last_detection_time is not None:
                self.detection_interval = current_time - self.last_detection_time
            self.last_detection_time = current_time
            
            # 1. ç›®æ ‡æ£€æµ‹
            detections = self._detect_objects(color_image, text_prompt)
            
            # 2. ä¸ºæ¯ä¸ªæ£€æµ‹ç»“æœæå–ç‚¹äº‘
            point_clouds = []
            for detection in detections:
                point_cloud_data = self._extract_point_cloud(color_image, depth_image, detection, camera_intrinsics)
                if point_cloud_data is not None:
                    point_clouds.append({
                        "detection": detection,
                        "point_cloud": point_cloud_data
                    })
            
            # 3. å¯è§†åŒ–ç»“æœ
            result_image = self._visualize_detections(color_image, detections)

            return {
                "success": True,
                "detections": detections,
                "point_clouds": point_clouds,
                "result_image": result_image
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _detect_objects(self, image: np.ndarray, text_prompt: str) -> List[Dict]:
        """
        ä½¿ç”¨GroundingDinoæ£€æµ‹ç›®æ ‡ç‰©ä½“
        
        Args:
            image: è¾“å…¥RGBå›¾åƒ (BGRæ ¼å¼ï¼ŒOpenCVæ ‡å‡†)
            text_prompt: ç›®æ ‡æè¿°æ–‡æœ¬
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦ã€æ ‡ç­¾å’Œæ©ç 
        """
        detections = []
        
        try:
            # æ ¼å¼åŒ–æ–‡æœ¬æç¤ºï¼ˆæ”¯æŒå¤šä¸ªç±»åˆ«ï¼‰ï¼Œå°†"."åˆ†éš”çš„ç±»åˆ«è½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(text_prompt, str):
                classes = [c.strip() for c in text_prompt.split(".") if c.strip()]
            else:
                classes = text_prompt
            
            # GroundingDinoæ£€æµ‹ï¼ˆéœ€è¦RGBæ ¼å¼ï¼‰
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨GroundingDinoè¿›è¡Œç›®æ ‡æ£€æµ‹
            print("ğŸ¤– ä½¿ç”¨çœŸå®GroundingDinoæ¨¡å‹æ£€æµ‹")
            detections_sv = self.grounding_dino_model.predict_with_classes(
                image=rgb_image,
                classes=classes,
                box_threshold=self.box_threshold,
                text_threshold=self.text_threshold
            )
            
            print(f"GroundingDinoæ£€æµ‹åˆ° {len(detections_sv.xyxy)} ä¸ªç›®æ ‡")
            
            if len(detections_sv.xyxy) == 0:
                return []
            
            # NMSåå¤„ç†
            import torch
            import torchvision
            nms_idx = torchvision.ops.nms(
                torch.from_numpy(detections_sv.xyxy),
                torch.from_numpy(detections_sv.confidence),
                self.nms_threshold
            ).numpy().tolist()
            
            # è¿‡æ»¤æ£€æµ‹ç»“æœ
            filtered_boxes = detections_sv.xyxy[nms_idx]
            filtered_confidences = detections_sv.confidence[nms_idx]
            filtered_class_ids = detections_sv.class_id[nms_idx]
            
            print(f"NMSåä¿ç•™ {len(filtered_boxes)} ä¸ªç›®æ ‡")
            
            # ä½¿ç”¨SAMç”Ÿæˆç²¾ç¡®æ©ç 
            masks = self._segment_with_sam(rgb_image, filtered_boxes)
            
            # æ ¼å¼åŒ–æ£€æµ‹ç»“æœ
            all_detections = []
            for i, (box, confidence, class_id, mask) in enumerate(
                zip(filtered_boxes, filtered_confidences, filtered_class_ids, masks)
            ):
                x1, y1, x2, y2 = box.astype(int)
                all_detections.append({
                    "bbox": [x1, y1, x2 - x1, y2 - y1],  # [x, y, width, height]
                    "xyxy": [x1, y1, x2, y2],  # [x1, y1, x2, y2]
                    "confidence": float(confidence),
                    "class_id": int(class_id),
                    "label": classes[class_id] if class_id < len(classes) else "object",
                    "mask": mask
                })
            
            # æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            detections = self._pick_best_detection_per_class(all_detections)
            
            return detections
            
        except Exception as e:
            print(f"æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            return []
    
    def _pick_best_detection_per_class(self, detections: List[Dict]) -> List[Dict]:
        """æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ"""
        if not detections:
            return detections
        
        # æŒ‰ç±»åˆ«æ ‡ç­¾åˆ†ç»„ï¼ˆç›´æ¥ä½¿ç”¨labelè€Œä¸æ˜¯class_idï¼‰
        class_groups = {}
        for detection in detections:
            label = detection["label"]
            if label not in class_groups:
                class_groups[label] = []
            class_groups[label].append(detection)
        
        # æ¯ä¸ªç±»åˆ«ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ª
        filtered_detections = []
        for label, group in class_groups.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–æœ€é«˜çš„
            best_detection = max(group, key=lambda x: x["confidence"])
            filtered_detections.append(best_detection)
            
            print(f"ğŸ“¦ ç±»åˆ« '{label}': ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ ({best_detection['confidence']:.3f})")
        
        return filtered_detections

    def _segment_with_sam(self, image: np.ndarray, boxes: np.ndarray) -> List[np.ndarray]:
        """ä½¿ç”¨SAMå¯¹æ£€æµ‹æ¡†è¿›è¡Œç²¾ç¡®åˆ†å‰²ï¼ˆæ¥è‡ªSAMçš„ä¾‹ç¨‹ï¼‰"""
        self.sam_predictor.set_image(image)
        result_masks = []
        
        for box in boxes:
            masks, scores, logits = self.sam_predictor.predict(
                box=box,
                multimask_output=True
            )
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ©ç 
            index = np.argmax(scores)
            result_masks.append(masks[index])
            
        return result_masks
    
    def _extract_point_cloud(self, color_image: np.ndarray, depth_image: np.ndarray, detection: Dict, camera_intrinsics: Optional[Dict[str, float]] = None) -> Optional[Dict]:
        """
        æ ¹æ®æ£€æµ‹ç»“æœæå–ç›®æ ‡ç‰©ä½“çš„ç‚¹äº‘ï¼ˆåŒ…å«é¢œè‰²ä¿¡æ¯ï¼‰
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            detection: æ£€æµ‹ç»“æœ
            camera_intrinsics: ç›¸æœºå†…å‚å­—å…¸
            
        Returns:
            åŒ…å«ç‚¹äº‘å’Œé¢œè‰²ä¿¡æ¯çš„å­—å…¸ {"points": (N, 3), "colors": (N, 3)} æˆ– None
        """
        try:
            # å¿…é¡»ä½¿ç”¨SAMç”Ÿæˆçš„maskï¼Œç¡®ä¿ç²¾ç¡®åˆ†å‰²
            mask = detection.get("mask")
            if mask is None:
                print("âš ï¸  è­¦å‘Š: æ£€æµ‹ç»“æœä¸­æ²¡æœ‰maskï¼Œè·³è¿‡ç‚¹äº‘æå–")
                return None
            
            h, w = color_image.shape[:2]
            print(f"æå–ç‚¹äº‘ï¼Œå›¾åƒå°ºå¯¸: {w}x{h}")
            
            # ä½¿ç”¨ä¼ å…¥çš„ç›¸æœºå†…å‚æˆ–é»˜è®¤å€¼
            if camera_intrinsics is not None:
                fx = camera_intrinsics["fx"]
                fy = camera_intrinsics["fy"]
                cx = camera_intrinsics["cx"]
                cy = camera_intrinsics["cy"]
            else:
                # é»˜è®¤ç›¸æœºå†…å‚
                fx, fy = 525.0, 525.0  # ç„¦è·
                cx, cy = w / 2.0, h / 2.0  # å…‰å¿ƒ
            
            points = []
            colors = []
            
            # éå†æ©ç åŒºåŸŸ - æ³¨æ„ï¼šnp.whereè¿”å›çš„æ˜¯(y_coords, x_coords)
            y_coords, x_coords = np.where(mask > 0)  # è¿™é‡Œæ˜¯å…ˆyåxï¼
            
            for y, x in zip(y_coords, x_coords):
                # è·å–æ·±åº¦å€¼
                depth = depth_image[y, x]  # æ³¨æ„ï¼šæ·±åº¦å›¾ç´¢å¼•æ˜¯[y, x]ï¼Œå³[è¡Œ, åˆ—]
                if depth > 0:  # æœ‰æ•ˆæ·±åº¦
                    # è½¬æ¢ä¸º3Dåæ ‡ (å•ä½: ç±³ï¼Œå‡è®¾æ·±åº¦å›¾å•ä½ä¸ºæ¯«ç±³)
                    z = depth / 1000.0
                    x_3d = (x - cx) * z / fx  # xå¯¹åº”åˆ—åæ ‡
                    y_3d = (y - cy) * z / fy  # yå¯¹åº”è¡Œåæ ‡
                    
                    points.append([x_3d, y_3d, z])
                    
                    # è·å–é¢œè‰² (BGRè½¬RGB)
                    b, g, r = color_image[y, x]  # åŒæ ·æ˜¯[è¡Œ, åˆ—]ç´¢å¼•
                    colors.append([r, g, b])
            
            if len(points) == 0:
                return None
            
            points = np.array(points, dtype=np.float32)
            colors = np.array(colors, dtype=np.uint8)

            return {
                "points": points,
                "colors": colors
            }
            
        except Exception as e:
            print(f"ç‚¹äº‘æå–å¤±è´¥: {e}")
            return None
    
    def _visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        result_image = image.copy()
        
        # åœ¨å›¾åƒé¡¶éƒ¨æ˜¾ç¤ºæ£€æµ‹æ—¶é—´é—´éš”
        if self.detection_interval > 0:
            fps = 1.0 / self.detection_interval if self.detection_interval > 0 else 0
            time_text = f"Detection Interval: {self.detection_interval:.3f}s ({fps:.1f} FPS)"
            cv2.putText(result_image, time_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        for i, detection in enumerate(detections):
            bbox = detection["bbox"]
            confidence = detection["confidence"]
            label = detection["label"]
            
            x, y, w, h = bbox
            
            # ä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†ä¸åŒç›®æ ‡
            colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦
            text = f"{label}: {confidence:.2f}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(result_image, (x, y + text_size[1]), 
                         (x + text_size[0], y), color, -1)
            cv2.putText(result_image, text, (x, y + text_size[1]), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ç»˜åˆ¶æ©ç ï¼ˆåŠé€æ˜ï¼‰
            if "mask" in detection and detection["mask"] is not None:
                mask = detection["mask"]
                if mask.dtype == bool:
                    mask = mask.astype(np.uint8) * 255
                    
                # åˆ›å»ºå½©è‰²æ©ç 
                colored_mask = np.zeros_like(result_image)
                colored_mask[mask > 0] = color
                result_image = cv2.addWeighted(result_image, 0.7, colored_mask, 0.3, 0)
        
        return result_image
    

    



class GroundingDinoROS2Node(Node):
    """åŸºäºROS2çš„GroundingDinoæ£€æµ‹èŠ‚ç‚¹"""
    
    def __init__(self, 
                 node_name: str = "grounding_dino_detector",
                 detection_prompt: str = "delivery box. pink takeout bag",
                 confidence_threshold: float = 0.4,
                 camera_intrinsics: Optional[Dict[str, float]] = None,
                 enable_image_visualization: bool = True,
                 enable_pointcloud_visualization: bool = False,
                 target_id_in_prompt: int = 1):
        """
        åˆå§‹åŒ–GroundingDino ROS2èŠ‚ç‚¹
        
        Args:
            node_name: èŠ‚ç‚¹åç§°
            detection_prompt: æ£€æµ‹ç›®æ ‡çš„æ–‡æœ¬æç¤º
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            camera_intrinsics: ç›¸æœºå†…å‚ {"fx": å€¼, "fy": å€¼, "cx": å€¼, "cy": å€¼}
            enable_image_visualization: æ˜¯å¦æ˜¾ç¤ºæ£€æµ‹ç»“æœçš„2Då›¾åƒçª—å£
            enable_pointcloud_visualization: æ˜¯å¦æ˜¾ç¤ºæŠ“å–ä½å§¿è®¡ç®—ä¸­çš„3Dç‚¹äº‘çª—å£
            target_id_in_prompt: ç‚¹äº‘å‘å¸ƒç›®æ ‡åœ¨promptä¸­çš„ç´¢å¼•
        """
        super().__init__(node_name)
        
        self.declare_parameter("grasp_food_pos_frame", 'grasp_food_pos')
        self.grasp_food_pos_frame = self.get_parameter("grasp_food_pos_frame").get_parameter_value().string_value
        
        # åˆå§‹åŒ–CV Bridge
        self.bridge = CvBridge()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.processor = AdvancedGroundingDinoProcessor()
        self.get_logger().info("ğŸ¤– ä½¿ç”¨GroundingDino+SAMæ¨¡å‹")

        # åˆå§‹åŒ–æŠ“å–ä½å§¿ä¼°è®¡å™¨
        self.grasp_estimator = GraspPoseEstimator(visualize=enable_pointcloud_visualization)
        pc_vis_status = "å¼€å¯" if enable_pointcloud_visualization else "å…³é—­"
        self.get_logger().info(f"ğŸ› ï¸  æŠ“å–ä½å§¿ä¼°è®¡å™¨å·²åˆå§‹åŒ– (3Dç‚¹äº‘å¯è§†åŒ–å·²{pc_vis_status})")

        # å®šä¹‰åæ ‡ç³»åç§°ï¼Œæ–¹ä¾¿ç®¡ç†
        self.robot_base_frame = 'nbman_base_link'  # ç¡®è®¤è¿™æ˜¯ä½ çš„æœºå™¨äººåŸºåº§æ ‡ç³»
        self.camera_frame = 'nbman_head_rgbd_color_optical_frame' # ç¡®è®¤è¿™æ˜¯ä½ çš„ç›¸æœºåæ ‡ç³»

        # åˆå§‹åŒ– TF2 Buffer å’Œ Listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)
            
        # é…ç½®å‚æ•°
        self.current_prompt = detection_prompt
        self.confidence_threshold = confidence_threshold
        self.enable_image_visualization = enable_image_visualization
        self.enable_pointcloud_visualization = enable_pointcloud_visualization
        
        # è§£æç”¨äºç‚¹äº‘å‘å¸ƒçš„ç›®æ ‡
        self.target_label = None
        prompt_classes = [c.strip() for c in self.current_prompt.split('.') if c.strip()]
        if 0 <= target_id_in_prompt < len(prompt_classes):
            self.target_label = prompt_classes[target_id_in_prompt]
            self.get_logger().info(f"â˜ï¸ ç‚¹äº‘å‘å¸ƒå’ŒæŠ“å–ä½å§¿è®¡ç®—ç›®æ ‡å·²è®¾ç½®ä¸º: '{self.target_label}'")
        else:
            self.get_logger().warn(f"âš ï¸ æ— æ•ˆçš„ target_id_in_prompt: {target_id_in_prompt}ã€‚å°†ä¸è¿›è¡ŒæŠ“å–ä½å§¿è®¡ç®—ã€‚")

        # ç›¸æœºå†…å‚è®¾ç½®
        if camera_intrinsics is None:
            # é»˜è®¤å†…å‚ï¼ˆéœ€è¦æ ¹æ®å®é™…ç›¸æœºè°ƒæ•´ï¼‰
            self.camera_intrinsics = {
                "fx": 848.0,  # ç„¦è·x
                "fy": 480.0,  # ç„¦è·y  
                "cx": 320.0,  # ä¸»ç‚¹x
                "cy": 240.0   # ä¸»ç‚¹y
            }
            self.get_logger().warn("âš ï¸  ä½¿ç”¨é»˜è®¤ç›¸æœºå†…å‚ï¼Œå»ºè®®ä¼ å…¥å®é™…å†…å‚")
        else:
            self.camera_intrinsics = camera_intrinsics
            
        self.get_logger().info(f"ğŸ“· ç›¸æœºå†…å‚: fx={self.camera_intrinsics['fx']}, fy={self.camera_intrinsics['fy']}")
        self.get_logger().info(f"ğŸ“· ä¸»ç‚¹: cx={self.camera_intrinsics['cx']}, cy={self.camera_intrinsics['cy']}")
        
        # å¤„ç†çŠ¶æ€
        self.frame_count = 0
        self.processing = False  # æ–°å¢ï¼šé˜²æ­¢å¹¶å‘å¤„ç†
        self.last_results = None
        
        # æ£€æµ‹ç»“æœå­˜å‚¨ - æŒ‰ç±»åˆ«åˆ†åˆ«å­˜å‚¨
        self.detected_objects = {}  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ {ç±»åˆ«å: [åå­—, ç½®ä¿¡åº¦, ç‚¹äº‘]}
        
        # å›¾åƒæ•°æ®ç¼“å­˜
        self.latest_color_image = None
        self.latest_depth_image = None
        
        # åˆ›å»ºè®¢é˜…è€…
        self.color_sub = self.create_subscription(
            Image,
            '/woosh/camera/woosh_left_hand_rgbd/color/image_raw',
            self.color_callback,
            10
        )
        
        self.depth_sub = self.create_subscription(
            Image,
            '/woosh/camera/woosh_left_hand_rgbd/aligned_depth_to_color/image_raw',
            self.depth_callback,
            10
        )

        # åˆ›å»ºæŠ“å–ä½å§¿å‘å¸ƒè€…
        self.grasp_pose_pub = self.create_publisher(
            PoseStamped,
            '/grounding_dino/grasp_pose',
            10
        )
        
        # åˆ›å»ºè°ƒè¯•ç‚¹äº‘å‘å¸ƒè€…
        self.debug_pc_pub = self.create_publisher(
            PointCloud2,
            '/grounding_dino/debug_pointcloud',
            10
        )
        
        
        self.get_logger().info("ğŸš€ GroundingDino ROS2èŠ‚ç‚¹å¯åŠ¨å®Œæˆ")
        self.get_logger().info("ğŸ“¡ è®¢é˜…è¯é¢˜:")
        self.get_logger().info("   RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
        self.get_logger().info("   æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
        self.get_logger().info("ğŸ“¤ å‘å¸ƒè¯é¢˜:")
        self.get_logger().info("   æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
        self.get_logger().info("   è°ƒè¯•ç‚¹äº‘: /grounding_dino/debug_pointcloud")
        self.get_logger().info(f"ğŸ¯ æ£€æµ‹ç›®æ ‡: '{self.current_prompt}'")
        self.get_logger().info(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
    
        
    def color_callback(self, msg: Image):
        """RGBå›¾åƒå›è°ƒå‡½æ•°"""
        try:
            self.latest_color_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.process_if_ready(self.latest_color_image, self.latest_depth_image)
            self.get_logger().info("âœ… è¿›å…¥color_callback")
                            
        except Exception as e:
            self.get_logger().error(f"RGBå›¾åƒå¤„ç†å¤±è´¥: {e}")

    def depth_callback(self, msg: Image):
        """æ·±åº¦å›¾åƒå›è°ƒå‡½æ•°"""
        try:
            self.latest_depth_image = self.bridge.imgmsg_to_cv2(msg, "16UC1")
            self.process_if_ready(self.latest_color_image, self.latest_depth_image)
            self.get_logger().info("âœ… è¿›å…¥depth_callback")
                 
        except Exception as e:
            self.get_logger().error(f"æ·±åº¦å›¾åƒå¤„ç†å¤±è´¥: {e}")
    
    def process_if_ready(self, cur_color_image: np.ndarray, cur_depth_image: np.ndarray):
        """æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„RGBDæ•°æ®ï¼Œå¦‚æœæœ‰åˆ™å¤„ç†"""
        if cur_color_image is None or cur_depth_image is None:
            return
        
        # é˜²æ­¢å¹¶å‘å¤„ç†
        if self.processing:
            return
            
        self.frame_count += 1
        
        # æ ¹æ®é…ç½®å†³å®šå¤„ç†é¢‘ç‡
        self.processing = True  # è®¾ç½®å¤„ç†æ ‡å¿—
            
        try:
            self.get_logger().info(f"ç¬¬ {self.frame_count} å¸§ - æ£€æµ‹ç›®æ ‡: '{self.current_prompt}'")
            
            # æ‰§è¡Œæ£€æµ‹
            self.last_results = self.processor.process(
                cur_color_image, 
                cur_depth_image, 
                self.current_prompt,
                camera_intrinsics=self.camera_intrinsics
            )
            
            if self.last_results["success"]:
                # å¯é€‰åœ°æ˜¾ç¤ºç»“æœå›¾åƒ
                if self.enable_image_visualization:
                    result_image = self.last_results["result_image"]
                    cv2.imshow("GroundingDino Results", result_image)
                    cv2.waitKey(1)

                # æ›´æ–°æ£€æµ‹ç»“æœ
                self._update_detection_results()
                
                # æ‰“å°æ£€æµ‹ä¿¡æ¯
                detections = self.last_results["detections"]
                if detections:
                    self.get_logger().info(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡:")
                    for i, det in enumerate(detections):
                        self.get_logger().info(f"  {i+1}. {det['label']}: {det['confidence']:.3f}")
                        # æ‰“å°ç‚¹äº‘ä¿¡æ¯
                        point_clouds = self.last_results["point_clouds"]
                        if i < len(point_clouds) and point_clouds[i]["point_cloud"] is not None:
                            pc_size = len(point_clouds[i]["point_cloud"]["points"])
                            self.get_logger().info(f"     ç‚¹äº‘å¤§å°: {pc_size} ä¸ªç‚¹")
                else:
                    self.get_logger().warn(f"âŒ æœªæ£€æµ‹åˆ°ç›®æ ‡: '{self.current_prompt}'")
            else:
                if self.enable_image_visualization:
                    cv2.imshow("GroundingDino Results", cur_color_image)
                    cv2.waitKey(1)
                self.get_logger().error(f"âŒ æ£€æµ‹å¤±è´¥: {self.last_results.get('error', 'Unknown error')}")
                self.last_results = None
                
        finally:
            self.processing = False  # æ¸…é™¤å¤„ç†æ ‡å¿—
            # å°†æ£€æµ‹ç»“æœç‚¹äº‘è½¬æ¢åˆ°æœºå™¨äººåæ ‡ç³»ä¸‹ï¼Œè®¡ç®—æŠ“å–ç‚¹
            if self.target_label and self.target_label in self.detected_objects:
                label, _, pointcloud_dict = self.detected_objects[self.target_label]
                
                if pointcloud_dict is not None and len(pointcloud_dict["points"]) > 0:
                    points_cam = pointcloud_dict["points"]
                    colors_cam = pointcloud_dict["colors"]
                    
                    self.get_logger().info(f"æ­£åœ¨ä¸º '{label}' è®¡ç®—æŠ“å–ä½å§¿...")
                    
                    # å°†ç‚¹äº‘è½¬æ¢åˆ°æœºå™¨äººåŸºåº§æ ‡ç³»
                    points_robot = self._transform_point_cloud(points_cam, self.camera_frame, self.robot_base_frame)

                    # æ£€æŸ¥è½¬æ¢æ˜¯å¦æˆåŠŸ
                    if points_robot is not None and points_robot.shape[0] > 0:
                        # # åˆ›å»ºå¹¶å‘å¸ƒè°ƒè¯•ç”¨çš„ç‚¹äº‘æ¶ˆæ¯
                        # debug_pc_msg = self._create_point_cloud_msg(points_robot, colors_cam, self.robot_base_frame)
                        # self.debug_pc_pub.publish(debug_pc_msg)
                        # self.get_logger().info("å·²å‘å¸ƒè°ƒè¯•ç‚¹äº‘åˆ° /grounding_dino/debug_pointcloud")

                        # ä½¿ç”¨GraspPoseEstimatorè®¡ç®—æŠ“å–ä½å§¿
                        grasp_pose_result = self.grasp_estimator.calculate_grasp_pose(points_robot, colors_cam)
                        
                        if grasp_pose_result:
                            grasp_point, grasp_orientation = grasp_pose_result
                            
                            # åˆ›å»ºå¹¶å‘å¸ƒPoseStampedæ¶ˆæ¯
                            pose_msg = PoseStamped()
                            pose_msg.header.stamp = self.get_clock().now().to_msg()
                            pose_msg.header.frame_id = self.robot_base_frame
                            pose_msg.pose.position = grasp_point
                            pose_msg.pose.orientation = grasp_orientation
                            
                            self.grasp_pose_pub.publish(pose_msg)
                            
                            # å‘å¸ƒè‡³TF
                            t = TransformStamped()
                            t.header.stamp = self.get_clock().now().to_msg()
                            t.header.frame_id = self.robot_base_frame
                            t.child_frame_id = self.grasp_food_pos_frame
                            t.transform.translation.x = grasp_point.x
                            t.transform.translation.y = grasp_point.y
                            t.transform.translation.z = grasp_point.z
                            t.transform.rotation.x = grasp_orientation.x
                            t.transform.rotation.y = grasp_orientation.y
                            t.transform.rotation.z = grasp_orientation.z
                            t.transform.rotation.w = grasp_orientation.w

                            self.tf_broadcaster.sendTransform(t)
                            self.get_logger().info(f"âœ… å·²å‘å¸ƒTFå˜æ¢: {self.robot_base_frame} -> {self.grasp_food_pos_frame}")
                            self.get_logger().info(f"âœ… æˆåŠŸå‘å¸ƒæŠ“å–ä½å§¿åˆ°è¯é¢˜ '{self.grasp_pose_pub.topic}'")
                    else:
                        self.get_logger().warn("ç‚¹äº‘åæ ‡è½¬æ¢å¤±è´¥æˆ–ç»“æœä¸ºç©ºï¼Œè·³è¿‡æŠ“å–è®¡ç®—")

    def _update_detection_results(self):
        """æ›´æ–°æ£€æµ‹ç»“æœåˆ°æˆå‘˜å˜é‡"""
        if not self.last_results or not self.last_results.get("success"):
            return
            
        detections = self.last_results.get("detections", [])
        point_clouds = self.last_results.get("point_clouds", [])
        
        # ç›´æ¥å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœï¼ˆå·²ç»æ˜¯æ¯ç±»æœ€ä½³çš„äº†ï¼‰
        for i, detection in enumerate(detections):
            label = detection.get("label", "unknown")
            confidence = detection.get("confidence", 0.0)
            
            # åªæ›´æ–°ç½®ä¿¡åº¦è¶…è¿‡é˜ˆå€¼ä¸”æœ‰ç‚¹äº‘çš„æ£€æµ‹
            if (confidence >= self.confidence_threshold and 
                i < len(point_clouds) and 
                point_clouds[i]["point_cloud"] is not None):
                
                # ç›´æ¥æ›´æ–°è¯¥ç±»åˆ«çš„æ£€æµ‹ç»“æœ
                self.detected_objects[label] = [
                    label,
                    confidence,
                    point_clouds[i]["point_cloud"]
                ]
                
                self.get_logger().info(f"ğŸ”„ æ›´æ–°ç±»åˆ« '{label}': ç½®ä¿¡åº¦ {confidence:.3f}")
            else:
                self.get_logger().debug(f" è·³è¿‡ '{label}': ç½®ä¿¡åº¦ {confidence:.3f} < {self.confidence_threshold} æˆ–æ— ç‚¹äº‘")
        
    def _transform_point_cloud(self, point_cloud_numpy: np.ndarray, source_frame: str, target_frame: str) -> Optional[np.ndarray]:
        """
        å°†ä¸€ä¸ªNumPyç‚¹äº‘ä»æºåæ ‡ç³»è½¬æ¢åˆ°ç›®æ ‡åæ ‡ç³»

        Args:
            point_cloud_numpy: (N, 3) çš„NumPyæ•°ç»„
            source_frame: æºåæ ‡ç³» (ä¾‹å¦‚ 'camera_color_optical_frame')
            target_frame: ç›®æ ‡åæ ‡ç³» (ä¾‹å¦‚ 'base_link')

        Returns:
            è½¬æ¢åçš„ (N, 3) NumPyæ•°ç»„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        if point_cloud_numpy.size == 0:
            return np.array([]) # å¦‚æœç‚¹äº‘ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºæ•°ç»„
            
        try:
            # 1. æŸ¥æ‰¾æœ€æ–°çš„å¯ç”¨å˜æ¢
            transform = self.tf_buffer.lookup_transform(
                target_frame,
                source_frame,
                rclpy.time.Time())

            # 2. é€ç‚¹è¿›è¡Œå˜æ¢
            # (å¯¹äºå¤§è§„æ¨¡ç‚¹äº‘æœ‰æ›´é«˜æ•ˆçš„æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•æœ€æ¸…æ™°ã€æœ€å¯é )
            transformed_points = []
            for point in point_cloud_numpy:
                # å°†NumPyç‚¹å°è£…æˆPointStampedæ¶ˆæ¯
                p_stamped = PointStamped()
                p_stamped.header.frame_id = source_frame
                p_stamped.point.x = float(point[0])
                p_stamped.point.y = float(point[1])
                p_stamped.point.z = float(point[2])

                # åº”ç”¨å˜æ¢
                p_transformed = tf2_geometry_msgs.do_transform_point(p_stamped, transform)
                
                transformed_points.append([
                    p_transformed.point.x,
                    p_transformed.point.y,
                    p_transformed.point.z
                ])
            
            return np.array(transformed_points, dtype=np.float32)

        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"åæ ‡å˜æ¢å¤±è´¥: ä» '{source_frame}' åˆ° '{target_frame}': {e}")
            return None

    def _create_point_cloud_msg(self, points: np.ndarray, colors: np.ndarray, frame_id: str) -> PointCloud2:
        """
        æ ¹æ®ç‚¹å’Œé¢œè‰²æ•°æ®åˆ›å»ºPointCloud2æ¶ˆæ¯
        """
        # 2. åˆ›å»ºä¸€ä¸ª Header å¯¹è±¡
        header = Header()
        header.stamp = self.get_clock().now().to_msg()
        header.frame_id = frame_id

        # å®šä¹‰ç‚¹äº‘å­—æ®µ
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1),
        ]
        
        # å°†é¢œè‰²(R,G,B)åˆå¹¶åˆ°ä¸€ä¸ªUINT32å­—æ®µä¸­
        colors_bgr = colors[:, [2, 1, 0]]
        cv2.imshow("Colors BGR", colors_bgr)
        rgb_packed = np.array((colors_bgr[:, 2] << 16) | (colors_bgr[:, 1] << 8) | (colors_bgr[:, 0]), dtype=np.uint32)
        
        # å°†ç‚¹å’Œé¢œè‰²æ•°æ®åˆå¹¶
        # åˆ›å»ºä¸€ä¸ªç»“æ„åŒ–æ•°ç»„
        point_data = np.zeros(points.shape[0], dtype=[
            ('x', np.float32),
            ('y', np.float32),
            ('z', np.float32),
            ('rgb', np.uint32)
        ])
        point_data['x'] = points[:, 0]
        point_data['y'] = points[:, 1]
        point_data['z'] = points[:, 2]
        point_data['rgb'] = rgb_packed

        # åˆ›å»ºPointCloud2æ¶ˆæ¯
        pc_msg = PointCloud2(
            header=header,
            height=1,
            width=points.shape[0],
            is_dense=True,
            is_bigendian=False,
            fields=fields,
            point_step=16, # 4 (x) + 4 (y) + 4 (z) + 4 (rgb)
            row_step=16 * points.shape[0],
            data=point_data.tobytes()
        )
        pc_msg.header.frame_id = frame_id
        
        return pc_msg

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.enable_image_visualization:
            cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ROS2èŠ‚ç‚¹"""
    rclpy.init()
    
    # å®é™…çš„ç›¸æœºå†…å‚ï¼ˆéœ€è¦æ ¹æ®æ‚¨çš„ç›¸æœºè°ƒæ•´ï¼‰
    camera_intrinsics = {
        "fx": 427.8312,  # å®é™…ç„¦è·x
        "fy": 427.3405,  # å®é™…ç„¦è·y
        "cx": 430.8444,  # å®é™…ä¸»ç‚¹x  
        "cy": 246.7171   # å®é™…ä¸»ç‚¹y
    }
    
    # åˆ›å»ºGroundingDinoæ£€æµ‹èŠ‚ç‚¹
    node = GroundingDinoROS2Node(
        node_name="grounding_dino_detector",
        detection_prompt="delivery box. pink takeout bag",
        confidence_threshold=0.4,
        camera_intrinsics=camera_intrinsics,
        enable_image_visualization=True,  # è®¾ç½®ä¸ºTrueå¯å¼€å¯2Då›¾åƒæ£€æµ‹ç»“æœçª—å£
        enable_pointcloud_visualization=False, # è®¾ç½®ä¸ºTrueå¯å¼€å¯3Dç‚¹äº‘å¤„ç†çª—å£
        target_id_in_prompt=1  # 0æ˜¯'delivery box', 1æ˜¯'pink takeout bag'
    )
    
    print("\n" + "="*60)
    print("ğŸ¯ GroundingDino + ROS2 æ£€æµ‹ç³»ç»Ÿ")
    print("="*60)
    print("ğŸ“¡ ROS2è¯é¢˜:")
    print("  è®¢é˜… RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
    print("  è®¢é˜… æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
    print("  å‘å¸ƒ æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
    print("="*60)
    print(f"â„¹ï¸  2Då›¾åƒå¯è§†åŒ–: {'å¯ç”¨' if node.enable_image_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  3Dç‚¹äº‘å¯è§†åŒ–: {'å¯ç”¨' if node.enable_pointcloud_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  æŠ“å–ç›®æ ‡: '{node.target_label}'")
    print("="*60 + "\n")
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        node.get_logger().error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        node.cleanup()
        node.destroy_node()
        rclpy.shutdown()

if __name__ == "__main__":
    main()