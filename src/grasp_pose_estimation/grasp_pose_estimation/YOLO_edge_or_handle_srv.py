"""
YOLOå¤„ç†å™¨çš„è¯¦ç»†å®ç°
åŒ…å«ç›®æ ‡æ£€æµ‹å’Œç‚¹äº‘æå–åŠŸèƒ½
é›†æˆYOLOv8æ¨¡å‹è¿›è¡Œå®ä¾‹åˆ†å‰²
åŸºäºROS2ç‰ˆæœ¬
"""

import torch
import numpy as np
import cv2
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import time
import math
from scipy.spatial.transform import Rotation as R

import rclpy
import message_filters
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField, CameraInfo
from geometry_msgs.msg import PoseStamped, TransformStamped
from std_msgs.msg import Header
from cv_bridge import CvBridge

# ------------[æœ¬åœ°æ¨¡å—å¯¼å…¥]------------
from .handle_grasp_pose_estimation import HandleGraspEstimator
from .grasp_pose_estimator import GraspPoseEstimator
import open3d as o3d
from image_to_grasp.srv import ImageToGrasp
# ------------[ç»“æŸæœ¬åœ°æ¨¡å—å¯¼å…¥]------------


import tf2_ros
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
from geometry_msgs.msg import PointStamped
import tf2_geometry_msgs # å¯¼å…¥å˜æ¢å‡½æ•°åº“
from tf2_ros import LookupException, ConnectivityException, ExtrapolationException





class YOLOProcessor:
    """YOLOå¤„ç†å™¨ - ä½¿ç”¨YOLOv8è¿›è¡Œç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²"""
    
    def __init__(self, 
                 model_path: str = "non_ros_pkg/YOLO/weights/best.pt",
                 conf_threshold: float = 0.25,
                 imgsz: int = 640,
                 device: str = "cuda"):
        """
        åˆå§‹åŒ–YOLOå¤„ç†å™¨
        
        Args:
            model_path: YOLOæ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºå·¥ä½œç©ºé—´æ ¹ç›®å½•ï¼‰
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            device: è®¡ç®—è®¾å¤‡ ("cuda" æˆ– "cpu")
        """
        self.conf_threshold = conf_threshold
        self.imgsz = imgsz
        
        # åˆå§‹åŒ–torchè®¾å¤‡
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # æ¨¡å‹è·¯å¾„
        # è·å–å½“å‰è„šæœ¬æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
        self.model_path = os.path.join(workspace_root, model_path)

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        
        # å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
        self.last_detection_time = None
        self.detection_interval = 0.0

        # åŠ è½½æ¨¡å‹
        self._load_models()
        
        print(f"YOLOå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _load_models(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        from ultralytics import YOLO

        # åŠ è½½YOLOæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½YOLOæ¨¡å‹: {self.model_path}")
        self.model = YOLO(self.model_path)
        print("YOLOæ¨¡å‹åŠ è½½æˆåŠŸ")

        # æ£€æŸ¥ CUDA å¯ç”¨æ€§å¹¶ç§»åŠ¨åˆ° GPU
        if torch.cuda.is_available():
            self.model.to('cuda')
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ° CUDA")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU æ¨¡å¼")

        # æ‰“å°æ¯ä¸ªæƒé‡çš„ device ä¿¡æ¯ï¼ˆå°½é‡å…¼å®¹ä¸åŒ YOLO å°è£…ï¼‰
        # try:
        #     torch_module = getattr(self.model, 'model', self.model)
        #     if hasattr(torch_module, 'named_parameters'):
        #         for name, param in torch_module.named_parameters():
        #             print(f"æƒé‡: {name} -> device: {param.device}")
        # except Exception as e:
        #     print(f"æ‰“å°æƒé‡è®¾å¤‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
    def process(self, color_image: np.ndarray, depth_image: np.ndarray, camera_intrinsics: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """
        å¤„ç†RGBDæ•°æ®, æ£€æµ‹ç›®æ ‡å¹¶æå–ç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            text_prompt: æ£€æµ‹ç›®æ ‡çš„æ–‡æœ¬æè¿°
            
        Returns:
            åŒ…å«æ£€æµ‹ç»“æœå’Œç‚¹äº‘çš„å­—å…¸
        """
        if color_image is None or depth_image is None:
            return {"success": False, "error": "Invalid image data"}
            
        try:
            # æ›´æ–°å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
            current_time = time.time()
            
            # è®¡ç®—æ£€æµ‹é—´éš”æ—¶é—´
            if self.last_detection_time is not None:
                self.detection_interval = current_time - self.last_detection_time
            self.last_detection_time = current_time
            
            # 1. ç›®æ ‡æ£€æµ‹
            detections = self._detect_objects(color_image)

            detect_time = time.time()
            print(f"YOLOæ£€æµ‹è€—æ—¶: {detect_time - current_time:.3f} ç§’")
            
            # 2. ä¸ºæ¯ä¸ªæ£€æµ‹ç»“æœæå–ç‚¹äº‘
            point_clouds = []
            for detection in detections:
                point_cloud_data = self._extract_point_cloud(color_image, depth_image, detection, camera_intrinsics)
                if point_cloud_data is not None:
                    point_clouds.append({
                        "detection": detection,
                        "point_cloud": point_cloud_data
                    })
            
            extract_time = time.time()
            print(f"ç‚¹äº‘æå–è€—æ—¶: {extract_time - detect_time:.3f} ç§’")
            # 3. å¯è§†åŒ–ç»“æœ
            result_image = self._visualize_detections(color_image, detections)

            finish_time = time.time()
            print(f"YOLOå¤„ç†æ€»è€—æ—¶: {finish_time - current_time:.3f} ç§’")

            return {
                "success": True,
                "detections": detections,
                "point_clouds": point_clouds,
                "result_image": result_image
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _detect_objects(self, image: np.ndarray) -> List[Dict]:
        """
        ä½¿ç”¨YOLOæ£€æµ‹ç›®æ ‡ç‰©ä½“
        
        Args:
            image: è¾“å…¥RGBå›¾åƒ (BGRæ ¼å¼ï¼ŒOpenCVæ ‡å‡†)
            text_prompt: ç›®æ ‡æè¿°æ–‡æœ¬ï¼ˆYOLOæ¨¡å¼ä¸‹æ­¤å‚æ•°ä¸ä½¿ç”¨ï¼Œå› ä¸ºæ£€æµ‹æ‰€æœ‰è®­ç»ƒçš„ç±»åˆ«ï¼‰
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦ã€æ ‡ç­¾å’Œæ©ç 
        """
        detections = []
        
        try:
            print("ğŸ¤– ä½¿ç”¨YOLOæ¨¡å‹æ£€æµ‹")
            
            self.imgsz = math.ceil(max(image.shape[:2]) / 32) * 32  # ç¡®ä¿æ˜¯32çš„å€æ•°
            # ä½¿ç”¨YOLOè¿›è¡Œæ¨ç†
            results = self.model.predict(
                source=image,
                imgsz=self.imgsz,
                conf=self.conf_threshold,
                verbose=False  # ç¦ç”¨è¯¦ç»†è¾“å‡º
            )
            
            # YOLOè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬åªå¤„ç†ç¬¬ä¸€ä¸ªç»“æœ
            if len(results) == 0:
                print("YOLOæœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡")
                return []
            
            result = results[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹ç»“æœ
            if result.masks is None or len(result.masks) == 0:
                print("YOLOæœªæ£€æµ‹åˆ°ä»»ä½•å¸¦æ©ç çš„ç›®æ ‡")
                return []
            
            print(f"YOLOæ£€æµ‹åˆ° {len(result.masks)} ä¸ªç›®æ ‡")
            
            # è·å–æ©ç æ•°æ®
            masks_data = result.masks.data.cpu().numpy()
            boxes = result.boxes.xyxy.cpu().numpy()
            confidences = result.boxes.conf.cpu().numpy()
            class_ids = result.boxes.cls.cpu().numpy().astype(int)
            names = result.names
            
            # è·å–åŸå§‹å›¾åƒå°ºå¯¸
            orig_h, orig_w = result.masks.orig_shape
            mask_h, mask_w = masks_data.shape[1:]
            print(f"åŸå§‹å›¾åƒå°ºå¯¸: {orig_w}x{orig_h}, æ©ç å°ºå¯¸: {mask_w}x{mask_h}")
            
            # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
            for i, (mask_padded, box, confidence, class_id) in enumerate(
                zip(masks_data, boxes, confidences, class_ids)
            ):
                # ------------ æ‰‹åŠ¨è°ƒæ•´æ©ç å°ºå¯¸ä»¥åŒ¹é…åŸå§‹å›¾åƒ ------------
                # æ­¥éª¤1: è®¡ç®—åŸå§‹å›¾åƒçš„å®½é«˜æ¯”
                orig_aspect = orig_w / orig_h
                # æ­¥éª¤2: è®¡ç®— YOLO ç¼©æ”¾åçš„å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
                if orig_aspect >= 1:  # å®½å›¾
                    scaled_w = self.imgsz
                    scaled_h = int(self.imgsz / orig_aspect)
                else:  # é«˜å›¾
                    scaled_h = self.imgsz
                    scaled_w = int(self.imgsz * orig_aspect)
                # æ­¥éª¤3: è®¡ç®— paddingï¼ˆYOLO ä¼šå°†å°ºå¯¸ pad åˆ°æœ€æ¥è¿‘çš„ stride å€æ•°ï¼Œé€šå¸¸æ˜¯32ï¼‰
                stride = 32
                padded_h = ((scaled_h + stride - 1) // stride) * stride
                padded_w = ((scaled_w + stride - 1) // stride) * stride
                # æ­¥éª¤4: å»é™¤ paddingï¼ˆè£å‰ªåˆ°ç¼©æ”¾åçš„å°ºå¯¸ï¼‰
                h_pad_total = padded_h - scaled_h
                w_pad_total = padded_w - scaled_w
                
                h_pad_top = h_pad_total // 2
                h_pad_bottom = h_pad_total - h_pad_top
                w_pad_left = w_pad_total // 2
                w_pad_right = w_pad_total - w_pad_left
                # è£å‰ªæ‰ padding
                if mask_h == padded_h and mask_w == padded_w:
                    # æ©ç å°ºå¯¸ä¸é¢„æœŸçš„ padded å°ºå¯¸åŒ¹é…
                    mask_unpadded = mask_padded[
                        h_pad_top:padded_h-h_pad_bottom,
                        w_pad_left:padded_w-w_pad_right
                    ]
                else:
                    # å¦‚æœä¸åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ©ç 
                    mask_unpadded = mask_padded
                print(f"å»é™¤ padding åæ©ç å½¢çŠ¶: {mask_unpadded.shape}")
                # æ­¥éª¤5: ç°åœ¨ resize åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                mask_resized = cv2.resize(mask_unpadded, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)
                print(f"æœ€ç»ˆæ©ç å½¢çŠ¶: {mask_resized.shape}")

                # äºŒå€¼åŒ–æ©ç 
                binary_mask = (mask_resized > 0.001).astype(np.uint8)
                
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = box.astype(int)
                
                # æ·»åŠ åˆ°æ£€æµ‹ç»“æœ
                detections.append({
                    "bbox": [x1, y1, x2 - x1, y2 - y1],  # [x, y, width, height]
                    "xyxy": [x1, y1, x2, y2],  # [x1, y1, x2, y2]
                    "confidence": float(confidence),
                    "class_id": int(class_id),
                    "label": names[class_id] if class_id < len(names) else f"class_{class_id}",
                    "mask": binary_mask
                })
            
            # æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            detections = self._pick_best_detection_per_class(detections)
            
            return detections
            
        except Exception as e:
            print(f"YOLOæ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _pick_best_detection_per_class(self, detections: List[Dict]) -> List[Dict]:
        """æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ"""
        if not detections:
            return detections
        
        # æŒ‰ç±»åˆ«æ ‡ç­¾åˆ†ç»„ï¼ˆç›´æ¥ä½¿ç”¨labelè€Œä¸æ˜¯class_idï¼‰
        class_groups = {}
        for detection in detections:
            label = detection["label"]
            if label not in class_groups:
                class_groups[label] = []
            class_groups[label].append(detection)
        
        # æ¯ä¸ªç±»åˆ«ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ª
        filtered_detections = []
        for label, group in class_groups.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–æœ€é«˜çš„
            best_detection = max(group, key=lambda x: x["confidence"])
            filtered_detections.append(best_detection)
            
            print(f"ğŸ“¦ ç±»åˆ« '{label}': ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ ({best_detection['confidence']:.3f})")
        
        return filtered_detections
    
    def _extract_point_cloud(self, color_image: np.ndarray, depth_image: np.ndarray, detection: Dict, camera_intrinsics: Optional[Dict[str, float]] = None) -> Optional[Dict]:
        """
        æ ¹æ®æ£€æµ‹ç»“æœæå–ç›®æ ‡ç‰©ä½“çš„ç‚¹äº‘ï¼ˆåŒ…å«é¢œè‰²ä¿¡æ¯ï¼‰
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            detection: æ£€æµ‹ç»“æœ
            camera_intrinsics: ç›¸æœºå†…å‚å­—å…¸
            
        Returns:
            åŒ…å«ç‚¹äº‘å’Œé¢œè‰²ä¿¡æ¯çš„å­—å…¸ {"points": (N, 3), "colors": (N, 3)} æˆ– None
        """
        try:
            mask = detection.get("mask")
            if mask is None: return None
            
            h, w = color_image.shape[:2]
            
            # 1. å‡†å¤‡å†…å‚
            if camera_intrinsics:
                fx = camera_intrinsics["fx"]
                fy = camera_intrinsics["fy"]
                cx = camera_intrinsics["cx"]
                cy = camera_intrinsics["cy"]
            else:
                fx, fy = 525.0, 525.0
                cx, cy = w / 2.0, h / 2.0

            points = []
            colors = []
            
            # # éå†æ©ç åŒºåŸŸ - æ³¨æ„ï¼šnp.whereè¿”å›çš„æ˜¯(y_coords, x_coords)
            # y_coords, x_coords = np.where(mask > 0)  # è¿™é‡Œæ˜¯å…ˆyåxï¼
            
            # for y, x in zip(y_coords, x_coords):
            #     # è·å–æ·±åº¦å€¼
            #     depth = depth_image[y, x]  # æ³¨æ„ï¼šæ·±åº¦å›¾ç´¢å¼•æ˜¯[y, x]ï¼Œå³[è¡Œ, åˆ—]
            #     if depth > 0:  # æœ‰æ•ˆæ·±åº¦
            #         # è½¬æ¢ä¸º3Dåæ ‡ (å•ä½: ç±³ï¼Œå‡è®¾æ·±åº¦å›¾å•ä½ä¸ºæ¯«ç±³)
            #         z = depth / 1000.0
            #         x_3d = (x - cx) * z / fx  # xå¯¹åº”åˆ—åæ ‡
            #         y_3d = (y - cy) * z / fy  # yå¯¹åº”è¡Œåæ ‡
                    
            #         points.append([x_3d, y_3d, z])
                    
            #         # è·å–é¢œè‰² (BGRè½¬RGB)
            #         b, g, r = color_image[y, x]  # åŒæ ·æ˜¯[è¡Œ, åˆ—]ç´¢å¼•
            #         colors.append([r, g, b])
            
            # if len(points) == 0:
            #     return None
            
            # points = np.array(points, dtype=np.float32)
            # colors = np.array(colors, dtype=np.uint8)
            
            # 2. è·å–æ©ç åŒºåŸŸçš„åæ ‡ç´¢å¼• (Vectorized)
            # np.where è¿”å›çš„æ˜¯ (row_indices, col_indices)ï¼Œå³ (y, x)
            v_idx, u_idx = np.where(mask > 0)
            
            if len(v_idx) == 0:
                return None

            # 3. æ‰¹é‡è·å–æ·±åº¦å€¼
            # åˆ©ç”¨é«˜çº§ç´¢å¼•ç›´æ¥æå–å‡ºæ‰€æœ‰æ©ç å†…çš„æ·±åº¦å€¼
            z_raw = depth_image[v_idx, u_idx]
            
            # 4. è¿‡æ»¤æ— æ•ˆæ·±åº¦ (æ·±åº¦ä¸º0çš„ç‚¹)
            # åˆ›å»ºä¸€ä¸ª boolean maskï¼Œåªä¿ç•™æ·±åº¦å¤§äº0çš„ç‚¹
            valid_mask = z_raw > 0
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œç›´æ¥è¿”å›
            if not np.any(valid_mask):
                return None
                
            # åº”ç”¨è¿‡æ»¤ï¼šåªä¿ç•™æœ‰æ•ˆçš„æ•°æ®
            z_raw = z_raw[valid_mask]
            u = u_idx[valid_mask]
            v = v_idx[valid_mask]
            
            # 5. æ ¸å¿ƒçŸ©é˜µè®¡ç®— (Vectorized Math)
            # å°†æ·±åº¦è½¬æ¢ä¸ºç±³
            z = z_raw / 1000.0
            
            # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç‚¹çš„ x å’Œ y
            x = (u - cx) * z / fx
            y = (v - cy) * z / fy
            
            # 6. å †å ä¸º (N, 3) æ•°ç»„
            # stack æŒ‰ç…§æœ€åä¸€ä¸ªç»´åº¦åˆå¹¶ï¼Œå½¢æˆ [ [x1,y1,z1], [x2,y2,z2], ... ]
            points = np.stack([x, y, z], axis=-1).astype(np.float32)
            
            # 7. æå–å¹¶å¤„ç†é¢œè‰²
            # åŒæ ·åˆ©ç”¨ç´¢å¼•æå–é¢œè‰²ï¼Œå¹¶ä» BGR è½¬ä¸º RGB
            colors_bgr = color_image[v, u] # æ³¨æ„è¿™é‡Œæ˜¯ v, u
            colors = colors_bgr[:, [2, 1, 0]].astype(np.uint8) # Swap BGR to RGB
            
            return {
                "points": points,
                "colors": colors
            }
            
        except Exception as e:
            print(f"ç‚¹äº‘æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        result_image = image.copy()
        
        # åœ¨å›¾åƒé¡¶éƒ¨æ˜¾ç¤ºæ£€æµ‹æ—¶é—´é—´éš”
        if self.detection_interval > 0:
            fps = 1.0 / self.detection_interval if self.detection_interval > 0 else 0
            time_text = f"Detection Interval: {self.detection_interval:.3f}s ({fps:.1f} FPS)"
            cv2.putText(result_image, time_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        for i, detection in enumerate(detections):
            bbox = detection["bbox"]
            confidence = detection["confidence"]
            label = detection["label"]
            
            x, y, w, h = bbox
            
            # ä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†ä¸åŒç›®æ ‡
            colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦
            text = f"{label}: {confidence:.2f}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(result_image, (x, y + text_size[1]), 
                         (x + text_size[0], y), color, -1)
            cv2.putText(result_image, text, (x, y + text_size[1]), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ç»˜åˆ¶æ©ç ï¼ˆåŠé€æ˜ï¼‰
            if "mask" in detection and detection["mask"] is not None:
                mask = detection["mask"]
                if mask.dtype == bool:
                    mask = mask.astype(np.uint8) * 255
                    
                # åˆ›å»ºå½©è‰²æ©ç 
                colored_mask = np.zeros_like(result_image)
                colored_mask[mask > 0] = color
                result_image = cv2.addWeighted(result_image, 0.7, colored_mask, 0.3, 0)
        
        return result_image
    

    



class YOLOServer(Node):
    """åŸºäºROS2çš„YOLOæ£€æµ‹èŠ‚ç‚¹"""
    
    def __init__(self, 
                 node_name: str = "yolo_detector",
                 model_path: str = "non_ros_pkg/YOLO/weights/best.pt",
                 confidence_threshold: float = 0.25,
                 imgsz: int = 640,
                 camera_intrinsics: Optional[Dict[str, float]] = None,
                 enable_image_visualization: bool = True,
                 enable_pointcloud_visualization: bool = False,
                 target_class_name: Optional[str] = None):
        """
        åˆå§‹åŒ–YOLO ROS2èŠ‚ç‚¹
        
        Args:
            node_name: èŠ‚ç‚¹åç§°
            model_path: YOLOæ¨¡å‹æƒé‡è·¯å¾„ï¼ˆç›¸å¯¹äºå·¥ä½œç©ºé—´æ ¹ç›®å½•ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            camera_intrinsics: ç›¸æœºå†…å‚ {"fx": å€¼, "fy": å€¼, "cx": å€¼, "cy": å€¼}
            enable_image_visualization: æ˜¯å¦æ˜¾ç¤ºæ£€æµ‹ç»“æœçš„2Då›¾åƒçª—å£
            enable_pointcloud_visualization: æ˜¯å¦æ˜¾ç¤ºæŠ“å–ä½å§¿è®¡ç®—ä¸­çš„3Dç‚¹äº‘çª—å£
            target_class_name: ç”¨äºç‚¹äº‘å‘å¸ƒå’ŒæŠ“å–ä½å§¿è®¡ç®—çš„ç›®æ ‡ç±»åˆ«åç§°
        """
        super().__init__(node_name)
        
        self.declare_parameter("edge_grasp_food_pos_frame", 'edge_grasp_food_pos')
        self.declare_parameter("handle_grasp_food_pos_frame", 'handle_grasp_food_pos')
        self.edge_grasp_food_pos_frame = self.get_parameter("edge_grasp_food_pos_frame").get_parameter_value().string_value
        self.handle_grasp_food_pos_frame = self.get_parameter("handle_grasp_food_pos_frame").get_parameter_value().string_value
        self.grasp_frame = self.edge_grasp_food_pos_frame  # é»˜è®¤ä½¿ç”¨è¾¹ç¼˜æŠ“å–ä½å§¿æ¡†æ¶
        # åˆå§‹åŒ–CV Bridge
        self.bridge = CvBridge()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.processor = YOLOProcessor(
            model_path=model_path,
            conf_threshold=confidence_threshold,
            imgsz=imgsz
        )
        self.get_logger().info("ğŸ¤– ä½¿ç”¨YOLOæ¨¡å‹")

        # åˆå§‹åŒ–æŠ“å–ä½å§¿ä¼°è®¡å™¨
        pc_vis_status = "å¼€å¯" if enable_pointcloud_visualization else "å…³é—­"
        self.handle_grasp_estimator = HandleGraspEstimator(
            voxel_size=0.001,              # ææ‰‹ç‚¹äº‘å†…éƒ¨å¤„ç†çš„ä½“ç´ å¤§å°
            dbscan_eps=0.02,               # ææ‰‹èšç±»Eps
            dbscan_min_points=30,
            hsv_v_max=0.2,                # é»‘è‰²/æ·±æ£•è‰²çš„äº®åº¦é˜ˆå€¼
            hsv_s_max=0.8,                 # é»‘è‰²/æ·±æ£•è‰²çš„é¥±å’Œåº¦é˜ˆå€¼
            u_shape_min_points=500,         # Uå½¢ç°‡æœ€å°ç‚¹æ•°
            u_shape_central_ratio=0.4,     # Uå½¢æ£€æµ‹ä¸­å¿ƒåŒºåŸŸæ¯”ä¾‹
            u_shape_hollow_ratio=0.10,     # Uå½¢ç©ºå¿ƒæ¯”ä¾‹
            grasp_bottom_height=0.03,      # æŠ“å–ç‚¹è®¡ç®—é«˜åº¦ (z_min + 0.03m)
            visualize=enable_pointcloud_visualization
        )
        self.edge_grasp_estimator = GraspPoseEstimator(visualize=enable_pointcloud_visualization)
        
        self.get_logger().info(f"ğŸ› ï¸  [Handle] Uå½¢ææ‰‹æŠ“å–ä¼°è®¡å™¨å·²åˆå§‹åŒ– (3Dç‚¹äº‘å¯è§†åŒ–å·²{pc_vis_status})")

        # å®šä¹‰åæ ‡ç³»åç§°ï¼Œæ–¹ä¾¿ç®¡ç†
        self.robot_base_frame = 'woosh_base_link'  # ç¡®è®¤è¿™æ˜¯ä½ çš„æœºå™¨äººåŸºåº§æ ‡ç³»
        self.camera_frame = 'woosh_left_hand_rgbd_depth_optical_frame' # ç¡®è®¤è¿™æ˜¯ä½ çš„ç›¸æœºåæ ‡ç³»

        # åˆå§‹åŒ– TF2 Buffer å’Œ Listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)
            
        # é…ç½®å‚æ•°
        self.target_label = target_class_name  # ç›´æ¥ä½¿ç”¨ç›®æ ‡ç±»åˆ«å
        self.confidence_threshold = confidence_threshold
        self.enable_image_visualization = enable_image_visualization
        self.enable_pointcloud_visualization = enable_pointcloud_visualization
        
        # ç›®æ ‡ç±»åˆ«è®¾ç½®
        if self.target_label:
            self.get_logger().info(f"â˜ï¸ ç‚¹äº‘å‘å¸ƒå’ŒæŠ“å–ä½å§¿è®¡ç®—ç›®æ ‡å·²è®¾ç½®ä¸º: '{self.target_label}'")
        else:
            self.get_logger().warn(f"âš ï¸ æœªè®¾ç½®ç›®æ ‡ç±»åˆ«åç§°ï¼Œå°†ä¸è¿›è¡ŒæŠ“å–ä½å§¿è®¡ç®—ã€‚")

        # ç›¸æœºå†…å‚è®¾ç½®
        if camera_intrinsics is None:
            # é»˜è®¤å†…å‚ï¼ˆéœ€è¦æ ¹æ®å®é™…ç›¸æœºè°ƒæ•´ï¼‰
            self.camera_intrinsics = {
                "fx": 848.0,  # ç„¦è·x
                "fy": 480.0,  # ç„¦è·y  
                "cx": 320.0,  # ä¸»ç‚¹x
                "cy": 240.0   # ä¸»ç‚¹y
            }
            self.get_logger().warn("âš ï¸  ä½¿ç”¨é»˜è®¤ç›¸æœºå†…å‚ï¼Œå»ºè®®ä¼ å…¥å®é™…å†…å‚")
        else:
            self.camera_intrinsics = camera_intrinsics
        self.has_camera_info = False  # æ ‡å¿—ä½ï¼Œè¡¨ç¤ºæ˜¯å¦å·²æ”¶åˆ°ç›¸æœºå†…å‚
        self.declare_parameter('camera_info_topic', '/woosh/camera/woosh_left_hand_rgbd/color/camera_info')
        camera_info_topic = self.get_parameter('camera_info_topic').get_parameter_value().string_value
        self.camera_info_sub = self.create_subscription(
            CameraInfo,
            camera_info_topic,
            self.camera_info_callback,
            10
        )

        self.get_logger().info(f"ğŸ“· ç›¸æœºå†…å‚: fx={self.camera_intrinsics['fx']}, fy={self.camera_intrinsics['fy']}")
        self.get_logger().info(f"ğŸ“· ä¸»ç‚¹: cx={self.camera_intrinsics['cx']}, cy={self.camera_intrinsics['cy']}")
        
        # å¤„ç†çŠ¶æ€
        self.frame_count = 0
        self.processing = False  # æ–°å¢ï¼šé˜²æ­¢å¹¶å‘å¤„ç†
        self.last_results = None
        
        # æ£€æµ‹ç»“æœå­˜å‚¨ - æŒ‰ç±»åˆ«åˆ†åˆ«å­˜å‚¨
        self.detected_objects = {}  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ {ç±»åˆ«å: [åå­—, ç½®ä¿¡åº¦, ç‚¹äº‘]}
        
        # ç‚¹äº‘ç´¯ç§¯å˜é‡ (ä¿æŒä¸å˜)
        self.accumulated_pcd = o3d.geometry.PointCloud()
        self.accumulation_voxel_size = 0.001
        self.target_detected_last_frame = False

        # 1. åˆ›å»º MessageFilter è®¢é˜…è€…
        # self.color_sub_filter = message_filters.Subscriber(
        #     self,
        #     Image,
        #     '/woosh/camera/woosh_left_hand_rgbd/color/image_raw'
        # )
        # self.depth_sub_filter = message_filters.Subscriber(
        #     self,
        #     Image,
        #     '/woosh/camera/woosh_left_hand_rgbd/aligned_depth_to_color/image_raw'
        # )

        self.grasp_service = self.create_service(
            ImageToGrasp,  # æ›¿æ¢ä¸ºä½ çš„æœåŠ¡æ¶ˆæ¯ç±»å‹
            "/takeout_detection/image_to_grasp",  # æœåŠ¡è¯é¢˜
            self.handle_grasp_request  # æœåŠ¡å›è°ƒå‡½æ•°
        )
        
        self.get_logger().info("âœ… GroundingDinoæŠ“å–æœåŠ¡ç«¯å¯åŠ¨å®Œæˆ")
        self.get_logger().info(f"æœåŠ¡è¯é¢˜: /takeout_detection/image_to_grasp")
        self.get_logger().info(f"TFå‘å¸ƒ: {self.robot_base_frame} â†’ {self.grasp_frame}")
    
        # 2. åˆ›å»ºæ—¶é—´åŒæ­¥å™¨ (ApproximateTimeSynchronizer)
        # slop=0.1 è¡¨ç¤ºå…è®¸ color å’Œ depth ä¹‹é—´æœ‰ 0.1s (100ms) çš„æ—¶é—´æˆ³å·®å¼‚
        # self.ts = message_filters.ApproximateTimeSynchronizer(
        #     [self.color_sub_filter, self.depth_sub_filter],
        #     queue_size=10,  # é˜Ÿåˆ—å¤§å°
        #     slop=0.1
        # )
        
        # 3. æ³¨å†ŒåŒæ­¥åçš„å›è°ƒå‡½æ•°
        # self.ts.registerCallback(self.synchronized_callback)

        # åˆ›å»ºæŠ“å–ä½å§¿å‘å¸ƒè€…
        self.edge_grasp_pose_pub = self.create_publisher(
            PoseStamped,
            '/food_detection/edge_grasp_pose',
            10
        )
        self.handle_grasp_pose_pub = self.create_publisher(
            PoseStamped,
            '/food_detection/handle_grasp_pose',
            10
        )
        
        self.get_logger().info("ğŸš€ YOLO ROS2èŠ‚ç‚¹å¯åŠ¨å®Œæˆ")
        self.get_logger().info("ğŸ“¡ è®¢é˜…è¯é¢˜:")
        self.get_logger().info("   RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
        self.get_logger().info("   æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
        self.get_logger().info("ğŸ“¤ å‘å¸ƒè¯é¢˜:")
        self.get_logger().info("   æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
        self.get_logger().info("   è°ƒè¯•ç‚¹äº‘: /grounding_dino/debug_pointcloud")
        if self.target_label:
            self.get_logger().info(f"ğŸ¯ æ£€æµ‹ç›®æ ‡ç±»åˆ«: '{self.target_label}'")
        # self.get_logger().info(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")

    def camera_info_callback(self, msg: CameraInfo):
        """ä»camera_infoè·å–çœŸå®çš„ç›¸æœºå†…å‚"""
        if not self.has_camera_info:
            K = msg.k  # ç›¸æœºå†…å‚çŸ©é˜µ (3x3)
            self.k_cam = np.array([
                [K[0], K[1], K[2]],
                [K[3], K[4], K[5]],
                [K[6], K[7], K[8]]
            ])
            self.has_camera_info = True
            
            self.get_logger().info(
                f"ğŸ“· Camera intrinsics received from camera_info: "
                f"fx={self.k_cam[0,0]:.2f}, fy={self.k_cam[1,1]:.2f}, "
                f"cx={self.k_cam[0,2]:.2f}, cy={self.k_cam[1,2]:.2f}"
            )
            self.cam_intrinsics = {
            "fx": self.k_cam[0,0],
            "fy": self.k_cam[1,1],
            "cx": self.k_cam[0,2],
            "cy": self.k_cam[1,2]
            }
            # è·å–ä¸€æ¬¡åå¯ä»¥å–æ¶ˆè®¢é˜…ï¼ˆå¯é€‰ï¼‰
            # self.destroy_subscription(self.camera_info_sub)

    def handle_grasp_request(self, request, response):
        """æœåŠ¡å›è°ƒå‡½æ•°ï¼šå¤„ç†å®¢æˆ·ç«¯çš„å›¾åƒè¯·æ±‚ï¼Œç”ŸæˆæŠ“å–ä½å§¿"""
        self.get_logger().info("ğŸ“¥ æ”¶åˆ°å®¢æˆ·ç«¯å›¾åƒè¯·æ±‚ï¼Œå¼€å§‹å¤„ç†...")
        
        try:
            # 1. è§£æå®¢æˆ·ç«¯è¯·æ±‚ä¸­çš„å›¾åƒï¼ˆRGB + æ·±åº¦ï¼‰
            # è½¬æ¢RGBå›¾åƒï¼ˆsensor_msgs/Image â†’ OpenCVï¼‰
            color_img = self.bridge.imgmsg_to_cv2(request.color_image, "bgr8")
            # è½¬æ¢æ·±åº¦å›¾åƒï¼ˆå‡è®¾æ·±åº¦å›¾æ ¼å¼ä¸º16UC1ï¼Œå•ä½mmï¼‰
            depth_img = self.bridge.imgmsg_to_cv2(request.depth_image, "16UC1")
            
            # 2. æ‰§è¡Œæ£€æµ‹å’Œç‚¹äº‘æå–ï¼ˆè°ƒç”¨å¤„ç†å™¨ï¼‰
            process_result = self.processor.process(
                color_image=color_img,
                depth_image=depth_img,
                # text_prompt=self.detect_prompt,
                camera_intrinsics=self.cam_intrinsics
            )
            
            if not process_result["success"]:
                response.success = False
                response.message = f"æ£€æµ‹å¤±è´¥: {process_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                self.get_logger().error(response.message)
                return response
            
            # 3. ç­›é€‰ç›®æ ‡ç‚¹äº‘ï¼ˆåªä¿ç•™ç›®æ ‡æ ‡ç­¾çš„ç‚¹äº‘ï¼‰
            target_pointcloud = None
            for pc_item in process_result["point_clouds"]:
                det_label = pc_item["detection"]["label"]
                det_conf = pc_item["detection"]["confidence"]
                if det_label == self.target_label and det_conf >= self.confidence_threshold:
                    target_pointcloud = pc_item["point_cloud"]
                    break
            
            if target_pointcloud is None:
                response.success = False
                response.message = f"æœªæ£€æµ‹åˆ°ç›®æ ‡æ ‡ç­¾: '{self.target_label}'ï¼ˆæˆ–ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼‰"
                self.get_logger().warn(response.message)
                return response
            
            # 4. åæ ‡è½¬æ¢ï¼šç›¸æœºå¸§ â†’ æœºå™¨äººåŸºåº§å¸§
            points_cam = target_pointcloud["points"]
            points_base = self._transform_point_cloud(points_cam, self.camera_frame, self.robot_base_frame)
            if points_base is None or len(points_base) == 0:
                response.success = False
                response.message = "ç‚¹äº‘åæ ‡è½¬æ¢å¤±è´¥ï¼ˆç›¸æœºâ†’åŸºåº§ï¼‰"
                self.get_logger().error(response.message)
                return response
            # 5. è®¡ç®—æŠ“å–ä½å§¿å¹¶ä¸”å‘å¸ƒæŠ“å–ä½å§¿åˆ°TFå’Œè¯é¢˜
            if request.grasp_type == "edge":
                self.grasp_frame = self.edge_grasp_food_pos_frame
                grasp_result = self.edge_grasp_estimator.calculate_grasp_pose(points_base, target_pointcloud["colors"])
                if not grasp_result:
                    response.success = False
                    response.message = "æŠ“å–ä½å§¿è®¡ç®—å¤±è´¥"
                    self.get_logger().error(response.message)
                    return response
                
                grasp_point, grasp_quat = grasp_result

                self._publish_grasp_tf(grasp_point, grasp_quat)
                self._publish_grasp_topic(grasp_point, grasp_quat, grasp_type="edge")
            elif request.grasp_type == "handle":
                self.grasp_frame = self.handle_grasp_food_pos_frame
                grasp_result = self.handle_grasp_estimator.calculate_grasp_pose(points_base, target_pointcloud["colors"])
                if not grasp_result:
                    response.success = False
                    response.message = "ææ‰‹æŠ“å–ä½å§¿è®¡ç®—å¤±è´¥"
                    self.get_logger().error(response.message)
                    return response
                
                grasp_point, grasp_quat = grasp_result

                self._publish_grasp_tf(grasp_point, grasp_quat)
                self._publish_grasp_topic(grasp_point, grasp_quat, grasp_type="handle")
            else:
                response.success = False
                response.message = f"æœªçŸ¥çš„æŠ“å–ç±»å‹: '{request.grasp_type}'"
                self.get_logger().error(response.message)
                return response
            
            # 7. æ„å»ºæœåŠ¡å“åº”ï¼ˆè¿”å›æŠ“å–ä½å§¿ç»™å®¢æˆ·ç«¯ï¼‰
            response.success = True
            response.message = f"æŠ“å–{request.grasp_type}ä½å§¿ç”ŸæˆæˆåŠŸ, å·²å‘å¸ƒTF: {self.robot_base_frame} â†’ {self.grasp_frame}"
            response.grasp_pose.position = grasp_point
            response.grasp_pose.orientation = grasp_quat
            
            self.get_logger().info(f"âœ… å¤„ç†å®Œæˆï¼{response.message}")
            return response
            
        except Exception as e:
            response.success = False
            response.message = f"æœåŠ¡å¤„ç†å¼‚å¸¸: {str(e)}"
            self.get_logger().error(f"âŒ æœåŠ¡å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            return response

        
    def _transform_point_cloud(self, point_cloud_numpy: np.ndarray, source_frame: str, target_frame: str) -> Optional[np.ndarray]:
        """
        å°†ä¸€ä¸ªNumPyç‚¹äº‘ä»æºåæ ‡ç³»è½¬æ¢åˆ°ç›®æ ‡åæ ‡ç³»

        Args:
            point_cloud_numpy: (N, 3) çš„NumPyæ•°ç»„
            source_frame: æºåæ ‡ç³» (ä¾‹å¦‚ 'camera_color_optical_frame')
            target_frame: ç›®æ ‡åæ ‡ç³» (ä¾‹å¦‚ 'base_link')

        Returns:
            è½¬æ¢åçš„ (N, 3) NumPyæ•°ç»„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        if point_cloud_numpy.size == 0:
            return np.array([]) # å¦‚æœç‚¹äº‘ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºæ•°ç»„
        
        # self.get_logger().info(f"è¯·æ±‚çš„æ—¶é—´æˆ³: {timestamp.sec}.{timestamp.nanosec}")
        self.get_logger().info(f"å½“å‰æ—¶é—´æˆ³: {self.get_clock().now().to_msg().sec}.{self.get_clock().now().to_msg().nanosec}")

        try:
            time_start = time.time()
            # 1. æŸ¥æ‰¾æŒ‡å®šæ—¶é—´æˆ³çš„å˜æ¢
            transform = self.tf_buffer.lookup_transform(
                target_frame,
                source_frame,
                rclpy.time.Time())
            time_end = time.time()
            self.get_logger().info(f"ğŸ”„ æŸ¥æ‰¾å˜æ¢è€—æ—¶: {time_end - time_start:.3f} ç§’")
            # transform = self.tf_buffer.lookup_transform(
            #     target_frame,
            #     source_frame,
            #     timestamp,  # <--- ä½¿ç”¨ä¼ å…¥çš„æ—¶é—´æˆ³
            #     timeout=rclpy.duration.Duration(seconds=0.1) # å¢åŠ ä¸€ä¸ªçŸ­æš‚è¶…æ—¶
            # )

            # 2. æå–å¹³ç§»å’Œæ—‹è½¬ (Scipyå¤„ç†)
            t = transform.transform.translation
            translation = np.array([t.x, t.y, t.z])

            q = transform.transform.rotation
            rotation = R.from_quat([q.x, q.y, q.z, q.w])

            # 3. çŸ©é˜µè¿ç®—åº”ç”¨å˜æ¢ (æ ¸å¿ƒåŠ é€Ÿéƒ¨åˆ†)
            time_start = time.time()
            # P_new = R * P_old + T
            transformed_points = rotation.apply(point_cloud_numpy) + translation
            
            time_end = time.time()
            self.get_logger().info(f"ğŸ”„ ç‚¹äº‘å˜æ¢è€—æ—¶(Vectorized): {time_end - time_start:.6f} ç§’")
            
            return transformed_points.astype(np.float32)

        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"åæ ‡å˜æ¢å¤±è´¥: ä» '{source_frame}' åˆ° '{target_frame}': {e}")
            return None

        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"åæ ‡å˜æ¢å¤±è´¥: ä» '{source_frame}' åˆ° '{target_frame}': {e}")
            return None

    def _publish_grasp_tf(self, grasp_point, grasp_quat):
        """å‘å¸ƒæŠ“å–ä½å§¿åˆ°TF"""
        t = TransformStamped()
        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = self.robot_base_frame
        t.child_frame_id = self.grasp_frame
        # ä½ç½®
        t.transform.translation.x = grasp_point.x
        t.transform.translation.y = grasp_point.y
        t.transform.translation.z = grasp_point.z
        # å§¿æ€ï¼ˆå››å…ƒæ•°ï¼‰
        t.transform.rotation.x = grasp_quat.x
        t.transform.rotation.y = grasp_quat.y
        t.transform.rotation.z = grasp_quat.z
        t.transform.rotation.w = grasp_quat.w
        
        self.tf_broadcaster.sendTransform(t)

    def _publish_grasp_topic(self, grasp_point, grasp_quat, grasp_type):
        """å‘å¸ƒæŠ“å–ä½å§¿åˆ°è¯é¢˜ï¼ˆå¯é€‰ï¼Œä¾›å…¶ä»–èŠ‚ç‚¹è®¢é˜…ï¼‰"""
        pose_msg = PoseStamped()
        pose_msg.header.stamp = self.get_clock().now().to_msg()
        pose_msg.header.frame_id = self.robot_base_frame
        pose_msg.pose.position = grasp_point
        pose_msg.pose.orientation = grasp_quat
        if grasp_type == "edge":
            self.edge_grasp_pose_pub.publish(pose_msg)

        if grasp_type == "handle":
            self.handle_grasp_pose_pub.publish(pose_msg)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.enable_image_visualization:
            cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ROS2èŠ‚ç‚¹"""
    rclpy.init()
    
    # å·¦æ‰‹ç›¸æœºå†…å‚
    camera_intrinsics = {
        "fx": 608.837158203125,  # å®é™…ç„¦è·x
        "fy": 609.1549682617188,  # å®é™…ç„¦è·y
        "cx": 424.99688720703125,  # å®é™…ä¸»ç‚¹x  
        "cy": 245.81431579589844   # å®é™…ä¸»ç‚¹y
    }
    # å³æ‰‹ç›¸æœºå†…å‚
    # camera_intrinsics = {
    #     "fx": 431.7814636230469,  # å®é™…ç„¦è·x
    #     "fy": 431.7814636230469,  # å®é™…ç„¦è·y
    #     "cx": 423.0641174316406,  # å®é™…ä¸»ç‚¹x  
    #     "cy": 235.52688598632812   # å®é™…ä¸»ç‚¹y
    # }
    
    # åˆ›å»ºYOLOæ£€æµ‹èŠ‚ç‚¹
    node = YOLOServer(
        node_name="yolo_detector",
        model_path="non_ros_pkg/YOLO/weights/best.pt",  # YOLOæ¨¡å‹è·¯å¾„
        confidence_threshold=0.25,  # YOLOç½®ä¿¡åº¦é˜ˆå€¼
        imgsz=640,  # è¾“å…¥å›¾åƒå°ºå¯¸
        camera_intrinsics=camera_intrinsics,
        enable_image_visualization=False,  # è®¾ç½®ä¸ºTrueå¯å¼€å¯2Då›¾åƒæ£€æµ‹ç»“æœçª—å£
        enable_pointcloud_visualization=False, # è®¾ç½®ä¸ºTrueå¯å¼€å¯3Dç‚¹äº‘å¤„ç†çª—å£
        target_class_name="takeout bag"  # è®¾ç½®ä½ è®­ç»ƒçš„YOLOæ¨¡å‹ä¸­çš„ç›®æ ‡ç±»åˆ«åç§°
    )
    
    print("\n" + "="*60)
    print("ğŸ¯ YOLO + ROS2 æ£€æµ‹ç³»ç»Ÿ")
    print("="*60)
    print("ğŸ“¡ ROS2è¯é¢˜:")
    print("  è®¢é˜… RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
    print("  è®¢é˜… æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
    print("  å‘å¸ƒ æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
    print("="*60)
    print(f"â„¹ï¸  2Då›¾åƒå¯è§†åŒ–: {'å¯ç”¨' if node.enable_image_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  3Dç‚¹äº‘å¯è§†åŒ–: {'å¯ç”¨' if node.enable_pointcloud_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  æŠ“å–ç›®æ ‡: '{node.target_label}'")
    print("="*60 + "\n")
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        node.get_logger().error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        node.cleanup()
        node.destroy_node()
        rclpy.shutdown()

if __name__ == "__main__":
    main()