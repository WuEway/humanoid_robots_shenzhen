"""
YOLOå¤„ç†å™¨çš„è¯¦ç»†å®ç°
åŒ…å«ç›®æ ‡æ£€æµ‹å’Œç‚¹äº‘æå–åŠŸèƒ½
é›†æˆYOLOv8æ¨¡å‹è¿›è¡Œå®ä¾‹åˆ†å‰²
åŸºäºROS2ç‰ˆæœ¬
"""

import torch
import numpy as np
import cv2
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import time
import math
from scipy.spatial.transform import Rotation as R

import rclpy
import message_filters
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField
from geometry_msgs.msg import PoseStamped, TransformStamped
from std_msgs.msg import Header
from cv_bridge import CvBridge

# ------------[æœ¬åœ°æ¨¡å—å¯¼å…¥]------------
from .handle_grasp_pose_estimation import HandleGraspEstimator
import open3d as o3d
# ------------[ç»“æŸæœ¬åœ°æ¨¡å—å¯¼å…¥]------------


import tf2_ros
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
from geometry_msgs.msg import PointStamped
import tf2_geometry_msgs # å¯¼å…¥å˜æ¢å‡½æ•°åº“
from tf2_ros import LookupException, ConnectivityException, ExtrapolationException





class YOLOProcessor:
    """YOLOå¤„ç†å™¨ - ä½¿ç”¨YOLOv8è¿›è¡Œç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²"""
    
    def __init__(self, 
                 model_path: str = "non_ros_pkg/YOLO/weights/best.pt",
                 conf_threshold: float = 0.25,
                 imgsz: int = 640,
                 device: str = "cuda"):
        """
        åˆå§‹åŒ–YOLOå¤„ç†å™¨
        
        Args:
            model_path: YOLOæ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºå·¥ä½œç©ºé—´æ ¹ç›®å½•ï¼‰
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            device: è®¡ç®—è®¾å¤‡ ("cuda" æˆ– "cpu")
        """
        self.conf_threshold = conf_threshold
        self.imgsz = imgsz
        
        # åˆå§‹åŒ–torchè®¾å¤‡
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # æ¨¡å‹è·¯å¾„
        # è·å–å½“å‰è„šæœ¬æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
        self.model_path = os.path.join(workspace_root, model_path)

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        
        # å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
        self.last_detection_time = None
        self.detection_interval = 0.0

        # åŠ è½½æ¨¡å‹
        self._load_models()
        
        print(f"YOLOå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _load_models(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        from ultralytics import YOLO

        # åŠ è½½YOLOæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½YOLOæ¨¡å‹: {self.model_path}")
        self.model = YOLO(self.model_path)
        print("YOLOæ¨¡å‹åŠ è½½æˆåŠŸ")

        # æ£€æŸ¥ CUDA å¯ç”¨æ€§å¹¶ç§»åŠ¨åˆ° GPU
        if torch.cuda.is_available():
            self.model.to('cuda')
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ° CUDA")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU æ¨¡å¼")

        # æ‰“å°æ¯ä¸ªæƒé‡çš„ device ä¿¡æ¯ï¼ˆå°½é‡å…¼å®¹ä¸åŒ YOLO å°è£…ï¼‰
        # try:
        #     torch_module = getattr(self.model, 'model', self.model)
        #     if hasattr(torch_module, 'named_parameters'):
        #         for name, param in torch_module.named_parameters():
        #             print(f"æƒé‡: {name} -> device: {param.device}")
        # except Exception as e:
        #     print(f"æ‰“å°æƒé‡è®¾å¤‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
    def process(self, color_image: np.ndarray, depth_image: np.ndarray, text_prompt: str = "object", camera_intrinsics: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """
        å¤„ç†RGBDæ•°æ®, æ£€æµ‹ç›®æ ‡å¹¶æå–ç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            text_prompt: æ£€æµ‹ç›®æ ‡çš„æ–‡æœ¬æè¿°
            
        Returns:
            åŒ…å«æ£€æµ‹ç»“æœå’Œç‚¹äº‘çš„å­—å…¸
        """
        if color_image is None or depth_image is None:
            return {"success": False, "error": "Invalid image data"}
            
        try:
            # æ›´æ–°å¸§è®¡æ•°å™¨å’Œæ—¶é—´è·Ÿè¸ª
            current_time = time.time()
            
            # è®¡ç®—æ£€æµ‹é—´éš”æ—¶é—´
            if self.last_detection_time is not None:
                self.detection_interval = current_time - self.last_detection_time
            self.last_detection_time = current_time
            
            # 1. ç›®æ ‡æ£€æµ‹
            detections = self._detect_objects(color_image, text_prompt)

            detect_time = time.time()
            print(f"YOLOæ£€æµ‹è€—æ—¶: {detect_time - current_time:.3f} ç§’")
            
            # 2. ä¸ºæ¯ä¸ªæ£€æµ‹ç»“æœæå–ç‚¹äº‘
            point_clouds = []
            for detection in detections:
                point_cloud_data = self._extract_point_cloud(color_image, depth_image, detection, camera_intrinsics)
                if point_cloud_data is not None:
                    point_clouds.append({
                        "detection": detection,
                        "point_cloud": point_cloud_data
                    })
            
            extract_time = time.time()
            print(f"ç‚¹äº‘æå–è€—æ—¶: {extract_time - detect_time:.3f} ç§’")
            # 3. å¯è§†åŒ–ç»“æœ
            result_image = self._visualize_detections(color_image, detections)

            finish_time = time.time()
            print(f"YOLOå¤„ç†æ€»è€—æ—¶: {finish_time - current_time:.3f} ç§’")

            return {
                "success": True,
                "detections": detections,
                "point_clouds": point_clouds,
                "result_image": result_image
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _detect_objects(self, image: np.ndarray, text_prompt: str) -> List[Dict]:
        """
        ä½¿ç”¨YOLOæ£€æµ‹ç›®æ ‡ç‰©ä½“
        
        Args:
            image: è¾“å…¥RGBå›¾åƒ (BGRæ ¼å¼ï¼ŒOpenCVæ ‡å‡†)
            text_prompt: ç›®æ ‡æè¿°æ–‡æœ¬ï¼ˆYOLOæ¨¡å¼ä¸‹æ­¤å‚æ•°ä¸ä½¿ç”¨ï¼Œå› ä¸ºæ£€æµ‹æ‰€æœ‰è®­ç»ƒçš„ç±»åˆ«ï¼‰
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦ã€æ ‡ç­¾å’Œæ©ç 
        """
        detections = []
        
        try:
            print("ğŸ¤– ä½¿ç”¨YOLOæ¨¡å‹æ£€æµ‹")
            
            self.imgsz = math.ceil(max(image.shape[:2]) / 32) * 32  # ç¡®ä¿æ˜¯32çš„å€æ•°
            # ä½¿ç”¨YOLOè¿›è¡Œæ¨ç†
            results = self.model.predict(
                source=image,
                imgsz=self.imgsz,
                conf=self.conf_threshold,
                verbose=False  # ç¦ç”¨è¯¦ç»†è¾“å‡º
            )
            
            # YOLOè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬åªå¤„ç†ç¬¬ä¸€ä¸ªç»“æœ
            if len(results) == 0:
                print("YOLOæœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡")
                return []
            
            result = results[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹ç»“æœ
            if result.masks is None or len(result.masks) == 0:
                print("YOLOæœªæ£€æµ‹åˆ°ä»»ä½•å¸¦æ©ç çš„ç›®æ ‡")
                return []
            
            print(f"YOLOæ£€æµ‹åˆ° {len(result.masks)} ä¸ªç›®æ ‡")
            
            # è·å–æ©ç æ•°æ®
            masks_data = result.masks.data.cpu().numpy()
            boxes = result.boxes.xyxy.cpu().numpy()
            confidences = result.boxes.conf.cpu().numpy()
            class_ids = result.boxes.cls.cpu().numpy().astype(int)
            names = result.names
            
            # è·å–åŸå§‹å›¾åƒå°ºå¯¸
            orig_h, orig_w = result.masks.orig_shape
            mask_h, mask_w = masks_data.shape[1:]
            print(f"åŸå§‹å›¾åƒå°ºå¯¸: {orig_w}x{orig_h}, æ©ç å°ºå¯¸: {mask_w}x{mask_h}")
            
            # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
            for i, (mask_padded, box, confidence, class_id) in enumerate(
                zip(masks_data, boxes, confidences, class_ids)
            ):
                # ------------ æ‰‹åŠ¨è°ƒæ•´æ©ç å°ºå¯¸ä»¥åŒ¹é…åŸå§‹å›¾åƒ ------------
                # æ­¥éª¤1: è®¡ç®—åŸå§‹å›¾åƒçš„å®½é«˜æ¯”
                orig_aspect = orig_w / orig_h
                # æ­¥éª¤2: è®¡ç®— YOLO ç¼©æ”¾åçš„å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
                if orig_aspect >= 1:  # å®½å›¾
                    scaled_w = self.imgsz
                    scaled_h = int(self.imgsz / orig_aspect)
                else:  # é«˜å›¾
                    scaled_h = self.imgsz
                    scaled_w = int(self.imgsz * orig_aspect)
                # æ­¥éª¤3: è®¡ç®— paddingï¼ˆYOLO ä¼šå°†å°ºå¯¸ pad åˆ°æœ€æ¥è¿‘çš„ stride å€æ•°ï¼Œé€šå¸¸æ˜¯32ï¼‰
                stride = 32
                padded_h = ((scaled_h + stride - 1) // stride) * stride
                padded_w = ((scaled_w + stride - 1) // stride) * stride
                # æ­¥éª¤4: å»é™¤ paddingï¼ˆè£å‰ªåˆ°ç¼©æ”¾åçš„å°ºå¯¸ï¼‰
                h_pad_total = padded_h - scaled_h
                w_pad_total = padded_w - scaled_w
                
                h_pad_top = h_pad_total // 2
                h_pad_bottom = h_pad_total - h_pad_top
                w_pad_left = w_pad_total // 2
                w_pad_right = w_pad_total - w_pad_left
                # è£å‰ªæ‰ padding
                if mask_h == padded_h and mask_w == padded_w:
                    # æ©ç å°ºå¯¸ä¸é¢„æœŸçš„ padded å°ºå¯¸åŒ¹é…
                    mask_unpadded = mask_padded[
                        h_pad_top:padded_h-h_pad_bottom,
                        w_pad_left:padded_w-w_pad_right
                    ]
                else:
                    # å¦‚æœä¸åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ©ç 
                    mask_unpadded = mask_padded
                print(f"å»é™¤ padding åæ©ç å½¢çŠ¶: {mask_unpadded.shape}")
                # æ­¥éª¤5: ç°åœ¨ resize åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                mask_resized = cv2.resize(mask_unpadded, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)
                print(f"æœ€ç»ˆæ©ç å½¢çŠ¶: {mask_resized.shape}")

                # äºŒå€¼åŒ–æ©ç 
                binary_mask = (mask_resized > 0.001).astype(np.uint8)
                
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = box.astype(int)
                
                # æ·»åŠ åˆ°æ£€æµ‹ç»“æœ
                detections.append({
                    "bbox": [x1, y1, x2 - x1, y2 - y1],  # [x, y, width, height]
                    "xyxy": [x1, y1, x2, y2],  # [x1, y1, x2, y2]
                    "confidence": float(confidence),
                    "class_id": int(class_id),
                    "label": names[class_id] if class_id < len(names) else f"class_{class_id}",
                    "mask": binary_mask
                })
            
            # æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            detections = self._pick_best_detection_per_class(detections)
            
            return detections
            
        except Exception as e:
            print(f"YOLOæ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _pick_best_detection_per_class(self, detections: List[Dict]) -> List[Dict]:
        """æ¯ä¸ªç±»åˆ«åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ"""
        if not detections:
            return detections
        
        # æŒ‰ç±»åˆ«æ ‡ç­¾åˆ†ç»„ï¼ˆç›´æ¥ä½¿ç”¨labelè€Œä¸æ˜¯class_idï¼‰
        class_groups = {}
        for detection in detections:
            label = detection["label"]
            if label not in class_groups:
                class_groups[label] = []
            class_groups[label].append(detection)
        
        # æ¯ä¸ªç±»åˆ«ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ª
        filtered_detections = []
        for label, group in class_groups.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–æœ€é«˜çš„
            best_detection = max(group, key=lambda x: x["confidence"])
            filtered_detections.append(best_detection)
            
            print(f"ğŸ“¦ ç±»åˆ« '{label}': ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ ({best_detection['confidence']:.3f})")
        
        return filtered_detections
    
    def _extract_point_cloud(self, color_image: np.ndarray, depth_image: np.ndarray, detection: Dict, camera_intrinsics: Optional[Dict[str, float]] = None) -> Optional[Dict]:
        """
        æ ¹æ®æ£€æµ‹ç»“æœæå–ç›®æ ‡ç‰©ä½“çš„ç‚¹äº‘ï¼ˆåŒ…å«é¢œè‰²ä¿¡æ¯ï¼‰
        
        Args:
            color_image: RGBå›¾åƒ (BGRæ ¼å¼)
            depth_image: æ·±åº¦å›¾åƒ
            detection: æ£€æµ‹ç»“æœ
            camera_intrinsics: ç›¸æœºå†…å‚å­—å…¸
            
        Returns:
            åŒ…å«ç‚¹äº‘å’Œé¢œè‰²ä¿¡æ¯çš„å­—å…¸ {"points": (N, 3), "colors": (N, 3)} æˆ– None
        """
        try:
            mask = detection.get("mask")
            if mask is None: return None
            
            h, w = color_image.shape[:2]
            
            # 1. å‡†å¤‡å†…å‚
            if camera_intrinsics:
                fx = camera_intrinsics["fx"]
                fy = camera_intrinsics["fy"]
                cx = camera_intrinsics["cx"]
                cy = camera_intrinsics["cy"]
            else:
                fx, fy = 525.0, 525.0
                cx, cy = w / 2.0, h / 2.0

            points = []
            colors = []
            
            # # éå†æ©ç åŒºåŸŸ - æ³¨æ„ï¼šnp.whereè¿”å›çš„æ˜¯(y_coords, x_coords)
            # y_coords, x_coords = np.where(mask > 0)  # è¿™é‡Œæ˜¯å…ˆyåxï¼
            
            # for y, x in zip(y_coords, x_coords):
            #     # è·å–æ·±åº¦å€¼
            #     depth = depth_image[y, x]  # æ³¨æ„ï¼šæ·±åº¦å›¾ç´¢å¼•æ˜¯[y, x]ï¼Œå³[è¡Œ, åˆ—]
            #     if depth > 0:  # æœ‰æ•ˆæ·±åº¦
            #         # è½¬æ¢ä¸º3Dåæ ‡ (å•ä½: ç±³ï¼Œå‡è®¾æ·±åº¦å›¾å•ä½ä¸ºæ¯«ç±³)
            #         z = depth / 1000.0
            #         x_3d = (x - cx) * z / fx  # xå¯¹åº”åˆ—åæ ‡
            #         y_3d = (y - cy) * z / fy  # yå¯¹åº”è¡Œåæ ‡
                    
            #         points.append([x_3d, y_3d, z])
                    
            #         # è·å–é¢œè‰² (BGRè½¬RGB)
            #         b, g, r = color_image[y, x]  # åŒæ ·æ˜¯[è¡Œ, åˆ—]ç´¢å¼•
            #         colors.append([r, g, b])
            
            # if len(points) == 0:
            #     return None
            
            # points = np.array(points, dtype=np.float32)
            # colors = np.array(colors, dtype=np.uint8)
            
            # 2. è·å–æ©ç åŒºåŸŸçš„åæ ‡ç´¢å¼• (Vectorized)
            # np.where è¿”å›çš„æ˜¯ (row_indices, col_indices)ï¼Œå³ (y, x)
            v_idx, u_idx = np.where(mask > 0)
            
            if len(v_idx) == 0:
                return None

            # 3. æ‰¹é‡è·å–æ·±åº¦å€¼
            # åˆ©ç”¨é«˜çº§ç´¢å¼•ç›´æ¥æå–å‡ºæ‰€æœ‰æ©ç å†…çš„æ·±åº¦å€¼
            z_raw = depth_image[v_idx, u_idx]
            
            # 4. è¿‡æ»¤æ— æ•ˆæ·±åº¦ (æ·±åº¦ä¸º0çš„ç‚¹)
            # åˆ›å»ºä¸€ä¸ª boolean maskï¼Œåªä¿ç•™æ·±åº¦å¤§äº0çš„ç‚¹
            valid_mask = z_raw > 0
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œç›´æ¥è¿”å›
            if not np.any(valid_mask):
                return None
                
            # åº”ç”¨è¿‡æ»¤ï¼šåªä¿ç•™æœ‰æ•ˆçš„æ•°æ®
            z_raw = z_raw[valid_mask]
            u = u_idx[valid_mask]
            v = v_idx[valid_mask]
            
            # 5. æ ¸å¿ƒçŸ©é˜µè®¡ç®— (Vectorized Math)
            # å°†æ·±åº¦è½¬æ¢ä¸ºç±³
            z = z_raw / 1000.0
            
            # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç‚¹çš„ x å’Œ y
            x = (u - cx) * z / fx
            y = (v - cy) * z / fy
            
            # 6. å †å ä¸º (N, 3) æ•°ç»„
            # stack æŒ‰ç…§æœ€åä¸€ä¸ªç»´åº¦åˆå¹¶ï¼Œå½¢æˆ [ [x1,y1,z1], [x2,y2,z2], ... ]
            points = np.stack([x, y, z], axis=-1).astype(np.float32)
            
            # 7. æå–å¹¶å¤„ç†é¢œè‰²
            # åŒæ ·åˆ©ç”¨ç´¢å¼•æå–é¢œè‰²ï¼Œå¹¶ä» BGR è½¬ä¸º RGB
            colors_bgr = color_image[v, u] # æ³¨æ„è¿™é‡Œæ˜¯ v, u
            colors = colors_bgr[:, [2, 1, 0]].astype(np.uint8) # Swap BGR to RGB
            
            return {
                "points": points,
                "colors": colors
            }
            
        except Exception as e:
            print(f"ç‚¹äº‘æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        result_image = image.copy()
        
        # åœ¨å›¾åƒé¡¶éƒ¨æ˜¾ç¤ºæ£€æµ‹æ—¶é—´é—´éš”
        if self.detection_interval > 0:
            fps = 1.0 / self.detection_interval if self.detection_interval > 0 else 0
            time_text = f"Detection Interval: {self.detection_interval:.3f}s ({fps:.1f} FPS)"
            cv2.putText(result_image, time_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        for i, detection in enumerate(detections):
            bbox = detection["bbox"]
            confidence = detection["confidence"]
            label = detection["label"]
            
            x, y, w, h = bbox
            
            # ä½¿ç”¨ä¸åŒé¢œè‰²åŒºåˆ†ä¸åŒç›®æ ‡
            colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦
            text = f"{label}: {confidence:.2f}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(result_image, (x, y + text_size[1]), 
                         (x + text_size[0], y), color, -1)
            cv2.putText(result_image, text, (x, y + text_size[1]), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ç»˜åˆ¶æ©ç ï¼ˆåŠé€æ˜ï¼‰
            if "mask" in detection and detection["mask"] is not None:
                mask = detection["mask"]
                if mask.dtype == bool:
                    mask = mask.astype(np.uint8) * 255
                    
                # åˆ›å»ºå½©è‰²æ©ç 
                colored_mask = np.zeros_like(result_image)
                colored_mask[mask > 0] = color
                result_image = cv2.addWeighted(result_image, 0.7, colored_mask, 0.3, 0)
        
        return result_image
    

    



class YOLOROS2Node(Node):
    """åŸºäºROS2çš„YOLOæ£€æµ‹èŠ‚ç‚¹"""
    
    def __init__(self, 
                 node_name: str = "yolo_detector",
                 model_path: str = "non_ros_pkg/YOLO/weights/best.pt",
                 confidence_threshold: float = 0.25,
                 imgsz: int = 640,
                 camera_intrinsics: Optional[Dict[str, float]] = None,
                 enable_image_visualization: bool = True,
                 enable_pointcloud_visualization: bool = False,
                 target_class_name: Optional[str] = None):
        """
        åˆå§‹åŒ–YOLO ROS2èŠ‚ç‚¹
        
        Args:
            node_name: èŠ‚ç‚¹åç§°
            model_path: YOLOæ¨¡å‹æƒé‡è·¯å¾„ï¼ˆç›¸å¯¹äºå·¥ä½œç©ºé—´æ ¹ç›®å½•ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            camera_intrinsics: ç›¸æœºå†…å‚ {"fx": å€¼, "fy": å€¼, "cx": å€¼, "cy": å€¼}
            enable_image_visualization: æ˜¯å¦æ˜¾ç¤ºæ£€æµ‹ç»“æœçš„2Då›¾åƒçª—å£
            enable_pointcloud_visualization: æ˜¯å¦æ˜¾ç¤ºæŠ“å–ä½å§¿è®¡ç®—ä¸­çš„3Dç‚¹äº‘çª—å£
            target_class_name: ç”¨äºç‚¹äº‘å‘å¸ƒå’ŒæŠ“å–ä½å§¿è®¡ç®—çš„ç›®æ ‡ç±»åˆ«åç§°
        """
        super().__init__(node_name)
        
        self.declare_parameter("grasp_food_pos_frame", 'grasp_food_pos')
        self.grasp_food_pos_frame = self.get_parameter("grasp_food_pos_frame").get_parameter_value().string_value
        
        # åˆå§‹åŒ–CV Bridge
        self.bridge = CvBridge()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.processor = YOLOProcessor(
            model_path=model_path,
            conf_threshold=confidence_threshold,
            imgsz=imgsz
        )
        self.get_logger().info("ğŸ¤– ä½¿ç”¨YOLOæ¨¡å‹")

        # åˆå§‹åŒ–æŠ“å–ä½å§¿ä¼°è®¡å™¨
        pc_vis_status = "å¼€å¯" if enable_pointcloud_visualization else "å…³é—­"
        self.grasp_estimator = HandleGraspEstimator(
            voxel_size=0.001,              # ææ‰‹ç‚¹äº‘å†…éƒ¨å¤„ç†çš„ä½“ç´ å¤§å°
            dbscan_eps=0.02,               # ææ‰‹èšç±»Eps
            dbscan_min_points=30,
            hsv_v_max=0.2,                # é»‘è‰²/æ·±æ£•è‰²çš„äº®åº¦é˜ˆå€¼
            hsv_s_max=0.8,                 # é»‘è‰²/æ·±æ£•è‰²çš„é¥±å’Œåº¦é˜ˆå€¼
            u_shape_min_points=500,         # Uå½¢ç°‡æœ€å°ç‚¹æ•°
            u_shape_central_ratio=0.4,     # Uå½¢æ£€æµ‹ä¸­å¿ƒåŒºåŸŸæ¯”ä¾‹
            u_shape_hollow_ratio=0.10,     # Uå½¢ç©ºå¿ƒæ¯”ä¾‹
            grasp_bottom_height=0.03,      # æŠ“å–ç‚¹è®¡ç®—é«˜åº¦ (z_min + 0.03m)
            visualize=enable_pointcloud_visualization
        )
        self.get_logger().info(f"ğŸ› ï¸  [Handle] Uå½¢ææ‰‹æŠ“å–ä¼°è®¡å™¨å·²åˆå§‹åŒ– (3Dç‚¹äº‘å¯è§†åŒ–å·²{pc_vis_status})")

        # å®šä¹‰åæ ‡ç³»åç§°ï¼Œæ–¹ä¾¿ç®¡ç†
        self.robot_base_frame = 'woosh_base_link'  # ç¡®è®¤è¿™æ˜¯ä½ çš„æœºå™¨äººåŸºåº§æ ‡ç³»
        self.camera_frame = 'woosh_left_hand_rgbd_depth_optical_frame' # ç¡®è®¤è¿™æ˜¯ä½ çš„ç›¸æœºåæ ‡ç³»

        # åˆå§‹åŒ– TF2 Buffer å’Œ Listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)
            
        # é…ç½®å‚æ•°
        self.target_label = target_class_name  # ç›´æ¥ä½¿ç”¨ç›®æ ‡ç±»åˆ«å
        self.confidence_threshold = confidence_threshold
        self.enable_image_visualization = enable_image_visualization
        self.enable_pointcloud_visualization = enable_pointcloud_visualization
        
        # ç›®æ ‡ç±»åˆ«è®¾ç½®
        if self.target_label:
            self.get_logger().info(f"â˜ï¸ ç‚¹äº‘å‘å¸ƒå’ŒæŠ“å–ä½å§¿è®¡ç®—ç›®æ ‡å·²è®¾ç½®ä¸º: '{self.target_label}'")
        else:
            self.get_logger().warn(f"âš ï¸ æœªè®¾ç½®ç›®æ ‡ç±»åˆ«åç§°ï¼Œå°†ä¸è¿›è¡ŒæŠ“å–ä½å§¿è®¡ç®—ã€‚")

        # ç›¸æœºå†…å‚è®¾ç½®
        if camera_intrinsics is None:
            # é»˜è®¤å†…å‚ï¼ˆéœ€è¦æ ¹æ®å®é™…ç›¸æœºè°ƒæ•´ï¼‰
            self.camera_intrinsics = {
                "fx": 848.0,  # ç„¦è·x
                "fy": 480.0,  # ç„¦è·y  
                "cx": 320.0,  # ä¸»ç‚¹x
                "cy": 240.0   # ä¸»ç‚¹y
            }
            self.get_logger().warn("âš ï¸  ä½¿ç”¨é»˜è®¤ç›¸æœºå†…å‚ï¼Œå»ºè®®ä¼ å…¥å®é™…å†…å‚")
        else:
            self.camera_intrinsics = camera_intrinsics
            
        self.get_logger().info(f"ğŸ“· ç›¸æœºå†…å‚: fx={self.camera_intrinsics['fx']}, fy={self.camera_intrinsics['fy']}")
        self.get_logger().info(f"ğŸ“· ä¸»ç‚¹: cx={self.camera_intrinsics['cx']}, cy={self.camera_intrinsics['cy']}")
        
        # å¤„ç†çŠ¶æ€
        self.frame_count = 0
        self.processing = False  # æ–°å¢ï¼šé˜²æ­¢å¹¶å‘å¤„ç†
        self.last_results = None
        
        # æ£€æµ‹ç»“æœå­˜å‚¨ - æŒ‰ç±»åˆ«åˆ†åˆ«å­˜å‚¨
        self.detected_objects = {}  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ {ç±»åˆ«å: [åå­—, ç½®ä¿¡åº¦, ç‚¹äº‘]}
        
        # ç‚¹äº‘ç´¯ç§¯å˜é‡ (ä¿æŒä¸å˜)
        self.accumulated_pcd = o3d.geometry.PointCloud()
        self.accumulation_voxel_size = 0.001
        self.target_detected_last_frame = False

        # 1. åˆ›å»º MessageFilter è®¢é˜…è€…
        self.color_sub_filter = message_filters.Subscriber(
            self,
            Image,
            '/woosh/camera/woosh_left_hand_rgbd/color/image_raw'
        )
        self.depth_sub_filter = message_filters.Subscriber(
            self,
            Image,
            '/woosh/camera/woosh_left_hand_rgbd/aligned_depth_to_color/image_raw'
        )

        # 2. åˆ›å»ºæ—¶é—´åŒæ­¥å™¨ (ApproximateTimeSynchronizer)
        # slop=0.1 è¡¨ç¤ºå…è®¸ color å’Œ depth ä¹‹é—´æœ‰ 0.1s (100ms) çš„æ—¶é—´æˆ³å·®å¼‚
        self.ts = message_filters.ApproximateTimeSynchronizer(
            [self.color_sub_filter, self.depth_sub_filter],
            queue_size=10,  # é˜Ÿåˆ—å¤§å°
            slop=0.1
        )
        
        # 3. æ³¨å†ŒåŒæ­¥åçš„å›è°ƒå‡½æ•°
        self.ts.registerCallback(self.synchronized_callback)

        # åˆ›å»ºæŠ“å–ä½å§¿å‘å¸ƒè€…
        self.grasp_pose_pub = self.create_publisher(
            PoseStamped,
            '/grounding_dino/grasp_pose',
            10
        )
        
        # åˆ›å»ºè°ƒè¯•ç‚¹äº‘å‘å¸ƒè€…
        self.debug_pc_pub = self.create_publisher(
            PointCloud2,
            '/grounding_dino/debug_pointcloud',
            10
        )
        
        
        self.get_logger().info("ğŸš€ YOLO ROS2èŠ‚ç‚¹å¯åŠ¨å®Œæˆ")
        self.get_logger().info("ğŸ“¡ è®¢é˜…è¯é¢˜:")
        self.get_logger().info("   RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
        self.get_logger().info("   æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
        self.get_logger().info("ğŸ“¤ å‘å¸ƒè¯é¢˜:")
        self.get_logger().info("   æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
        self.get_logger().info("   è°ƒè¯•ç‚¹äº‘: /grounding_dino/debug_pointcloud")
        if self.target_label:
            self.get_logger().info(f"ğŸ¯ æ£€æµ‹ç›®æ ‡ç±»åˆ«: '{self.target_label}'")
        self.get_logger().info(f"ğŸšï¸  ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")

    def synchronized_callback(self, color_msg: Image, depth_msg: Image):
        """
        åŒæ­¥çš„RGBå’ŒDepthæ¶ˆæ¯çš„å›è°ƒå‡½æ•°
        è¿™æ˜¯å¤„ç† pipeline çš„å”¯ä¸€å…¥å£
        """
        
        # è§£å†³ 2s å»¶è¿Ÿçš„å…³é”®ï¼šå¦‚æœæ­£åœ¨å¤„ç†ï¼Œç«‹å³ä¸¢å¼ƒæ–°å¸§
        if self.processing:
            self.get_logger().warn("å¤„ç†å™¨æ­£å¿™ (è€—æ—¶2s)ï¼Œè·³è¿‡æ­¤å¸§", throttle_duration_sec=2.0)
            return
            
        self.processing = True  # <--- è®¾ç½®å¤„ç†é”
        self.get_logger().info("âœ… æ”¶åˆ°åŒæ­¥å¸§ï¼Œå¼€å§‹å¤„ç†...")

        try:
            # 1. è½¬æ¢æ•°æ®
            cur_color_image = self.bridge.imgmsg_to_cv2(color_msg, "bgr8")
            cur_depth_image = self.bridge.imgmsg_to_cv2(depth_msg, "16UC1")
            
            # 2. é”å®šæ—¶é—´æˆ³ (æˆ‘ä»¬ä½¿ç”¨ color msg çš„æ—¶é—´æˆ³ä½œä¸ºåŸºå‡†)
            cur_stamp = color_msg.header.stamp

            # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†é€»è¾‘
            time_start = time.time()
            self.process_frame_data(cur_color_image, cur_depth_image, cur_stamp)
            time_end = time.time()
            self.get_logger().info(f"å¸§å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time_end - time_start:.3f} ç§’")

        except Exception as e:
            self.get_logger().error(f"åœ¨ synchronized_callback ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        finally:
            self.processing = False # <--- é‡Šæ”¾å¤„ç†é”
    
        
    def process_frame_data(self, cur_color_image: np.ndarray, cur_depth_image: np.ndarray, cur_stamp: rclpy.time.Time):
        """
        å¤„ç†ä¸€å¸§åŒæ­¥å¥½çš„RGBDæ•°æ®
        """
            
        self.frame_count += 1
        
        try:
            self.get_logger().info(f"ç¬¬ {self.frame_count} å¸§ - ä½¿ç”¨YOLOæ£€æµ‹ (Stamp: {cur_stamp.sec}.{cur_stamp.nanosec})")
            
            # æ‰§è¡Œæ£€æµ‹ (YOLOä¸éœ€è¦text_promptå‚æ•°ï¼Œä½†ä¸ºäº†å…¼å®¹ä¿ç•™)
            self.last_results = self.processor.process(
                cur_color_image, 
                cur_depth_image, 
                "",  # YOLOä¸ä½¿ç”¨æ–‡æœ¬æç¤º
                camera_intrinsics=self.camera_intrinsics
            )

            time_detect_end = time.time()
            
            if self.last_results["success"]:
                # å¯é€‰åœ°æ˜¾ç¤ºç»“æœå›¾åƒ
                if self.enable_image_visualization:
                    result_image = self.last_results["result_image"]
                    cv2.imshow("YOLO Results", result_image)
                    cv2.waitKey(1)

                # æ›´æ–°æ£€æµ‹ç»“æœ
                self._update_detection_results()
                
                # æ‰“å°æ£€æµ‹ä¿¡æ¯
                detections = self.last_results["detections"]
                if detections:
                    self.get_logger().info(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡:")
                    for i, det in enumerate(detections):
                        self.get_logger().info(f"  {i+1}. {det['label']}: {det['confidence']:.3f}")
                        # æ‰“å°ç‚¹äº‘ä¿¡æ¯
                        point_clouds = self.last_results["point_clouds"]
                        if i < len(point_clouds) and point_clouds[i]["point_cloud"] is not None:
                            pc_size = len(point_clouds[i]["point_cloud"]["points"])
                            self.get_logger().info(f"     ç‚¹äº‘å¤§å°: {pc_size} ä¸ªç‚¹")
                else:
                    self.get_logger().warn(f"âŒ YOLOæœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡")
            else:
                if self.enable_image_visualization:
                    cv2.imshow("YOLO Results", cur_color_image)
                    cv2.waitKey(1)
                self.get_logger().error(f"âŒ æ£€æµ‹å¤±è´¥: {self.last_results.get('error', 'Unknown error')}")
                self.last_results = None
            time_process_end = time.time()
            self.get_logger().info(f"ğŸ•’ ä¿å­˜ç»“æœè€—æ—¶: {time_process_end - time_detect_end:.3f} ç§’")
                
        finally:
            target_detected_this_frame = False

            # å°†æ£€æµ‹ç»“æœç‚¹äº‘è½¬æ¢åˆ°æœºå™¨äººåæ ‡ç³»ä¸‹ï¼Œè®¡ç®—æŠ“å–ç‚¹
            if self.target_label and self.target_label in self.detected_objects:
                label, _, pointcloud_dict = self.detected_objects[self.target_label]
                
                if pointcloud_dict is not None and len(pointcloud_dict["points"]) > 0:
                    points_cam = pointcloud_dict["points"]
                    colors_cam = pointcloud_dict["colors"]
                    
                    self.get_logger().info(f"ğŸ“¦ å‘ç°ç›®æ ‡ '{label}'ï¼Œæ­£åœ¨è½¬æ¢å¹¶ç´¯ç§¯ç‚¹äº‘...")
                    
                    time_start_transform = time.time()
                    # 1. å°†å½“å‰å¸§çš„ç‚¹äº‘è½¬æ¢åˆ°æœºå™¨äººåŸºåº§æ ‡ç³»
                    points_robot = self._transform_point_cloud(
                        points_cam, 
                        self.camera_frame, 
                        self.robot_base_frame,
                        cur_stamp  
                    )
                    time_end_transform = time.time()
                    self.get_logger().info(f"ğŸ”„ ç‚¹äº‘è½¬æ¢è€—æ—¶: {time_end_transform - time_start_transform}ç§’")
                    # æ£€æŸ¥è½¬æ¢æ˜¯å¦æˆåŠŸ
                    if points_robot is not None and points_robot.shape[0] > 0:
                        target_detected_this_frame = True

                        # 2. åˆ›å»ºå½“å‰å¸§çš„ Open3D ç‚¹äº‘
                        time_start = time.time()
                        current_pcd = o3d.geometry.PointCloud()
                        current_pcd.points = o3d.utility.Vector3dVector(points_robot)
                        current_pcd.colors = o3d.utility.Vector3dVector(colors_cam / 255.0) # é¢œè‰²è½¬ä¸º 0-1
                        time_end = time.time()
                        self.get_logger().info(f"ğŸ•’ å½“å‰ç‚¹äº‘åˆ›å»ºè€—æ—¶{time_end - time_start}ç§’")

                        # ä¿å­˜å½“å‰ç‚¹äº‘
                        self.get_logger().info(f"â˜ï¸ å½“å‰ç‚¹äº‘å¤§å°: {len(current_pcd.points)} ç‚¹")

                        # 3. ç´¯ç§¯ç‚¹äº‘
                        time_accum_start = time.time()
                        self.accumulated_pcd = current_pcd
                        # self.accumulated_pcd = self.accumulated_pcd.voxel_down_sample(self.accumulation_voxel_size)
                        time_accum_end = time.time()
                        self.get_logger().info(f"ğŸ•’ ç´¯ç§¯ç‚¹äº‘æå–è€—æ—¶{time_accum_end - time_accum_start}ç§’")
                        
                        self.get_logger().info(f"â˜ï¸ ç´¯ç§¯ç‚¹äº‘å¤§å°: {len(self.accumulated_pcd.points)} ç‚¹")

                        # 4. æå–ç´¯ç§¯çš„ç‚¹å’Œé¢œè‰²
                        acc_points = np.asarray(self.accumulated_pcd.points)
                        acc_colors = (np.asarray(self.accumulated_pcd.colors) * 255.0).astype(np.uint8) # é¢œè‰²è½¬å› 0-255

                        # # --- å¯é€‰: å‘å¸ƒç´¯ç§¯çš„è°ƒè¯•ç‚¹äº‘ ---
                        debug_pc_msg = self._create_point_cloud_msg(acc_points, acc_colors, self.robot_base_frame)
                        self.debug_pc_pub.publish(debug_pc_msg)
                        self.get_logger().info("å·²å‘å¸ƒ [ç´¯ç§¯] è°ƒè¯•ç‚¹äº‘")
                        time_debug_pub_end = time.time()
                        self.get_logger().info(f"ğŸ•’ è°ƒè¯•ç‚¹äº‘å‘å¸ƒè€—æ—¶{time_debug_pub_end - time_accum_end}ç§’")

                        # 5. ä½¿ç”¨GraspPoseEstimatorè®¡ç®—æŠ“å–ä½å§¿ (åœ¨ç´¯ç§¯ç‚¹äº‘ä¸Š)
                        t_start = time.time()
                        grasp_pose_result = self.grasp_estimator.calculate_grasp_pose(acc_points, acc_colors)
                        t_end = time.time()
                        self.get_logger().info(f"ğŸ› ï¸[Handle] æŠ“å–ä½å§¿è®¡ç®—è€—æ—¶: {t_end - t_start:.3f} ç§’")
                        
                        if grasp_pose_result:
                            grasp_point, grasp_orientation = grasp_pose_result
                            
                            # åˆ›å»ºå¹¶å‘å¸ƒPoseStampedæ¶ˆæ¯
                            pose_msg = PoseStamped()
                            pose_msg.header.stamp = self.get_clock().now().to_msg()
                            pose_msg.header.frame_id = self.robot_base_frame
                            pose_msg.pose.position = grasp_point
                            pose_msg.pose.orientation = grasp_orientation
                            
                            self.grasp_pose_pub.publish(pose_msg)
                            
                            # å‘å¸ƒè‡³TF
                            t = TransformStamped()
                            t.header.stamp = self.get_clock().now().to_msg()
                            t.header.frame_id = self.robot_base_frame
                            t.child_frame_id = self.grasp_food_pos_frame
                            t.transform.translation.x = grasp_point.x
                            t.transform.translation.y = grasp_point.y
                            t.transform.translation.z = grasp_point.z
                            t.transform.rotation.x = grasp_orientation.x
                            t.transform.rotation.y = grasp_orientation.y
                            t.transform.rotation.z = grasp_orientation.z
                            t.transform.rotation.w = grasp_orientation.w

                            self.tf_broadcaster.sendTransform(t)
                            self.get_logger().info(f"âœ… [Handle] å·²å‘å¸ƒTFå˜æ¢: {self.robot_base_frame} -> {self.grasp_food_pos_frame}")
                            self.get_logger().info(f"âœ… [Handle] æˆåŠŸå‘å¸ƒæŠ“å–ä½å§¿åˆ°è¯é¢˜ '{self.grasp_pose_pub.topic}'")
                    else:
                        self.get_logger().warn("æŠ“å–ä½å§¿è®¡ç®—å¤±è´¥æˆ–ç»“æœä¸ºç©ºï¼Œè·³è¿‡å‘å¸ƒTFå’Œè¯é¢˜")

            # 6. æ£€æŸ¥ç›®æ ‡æ˜¯å¦ä¸¢å¤±ï¼Œå¦‚æœä¸¢å¤±åˆ™æ¸…é™¤ç´¯ç§¯ç‚¹äº‘
            if not target_detected_this_frame and self.target_detected_last_frame:
                self.get_logger().warn("ğŸ¯ ç›®æ ‡ä¸¢å¤±! æ¸…é™¤ç´¯ç§¯ç‚¹äº‘ã€‚")
                self.accumulated_pcd = o3d.geometry.PointCloud()
            
            self.target_detected_last_frame = target_detected_this_frame

    def _update_detection_results(self):
        """æ›´æ–°æ£€æµ‹ç»“æœåˆ°æˆå‘˜å˜é‡"""
        if not self.last_results or not self.last_results.get("success"):
            return
            
        detections = self.last_results.get("detections", [])
        point_clouds = self.last_results.get("point_clouds", [])
        
        # ç›´æ¥å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœï¼ˆå·²ç»æ˜¯æ¯ç±»æœ€ä½³çš„äº†ï¼‰
        for i, detection in enumerate(detections):
            label = detection.get("label", "unknown")
            confidence = detection.get("confidence", 0.0)
            
            # åªæ›´æ–°ç½®ä¿¡åº¦è¶…è¿‡é˜ˆå€¼ä¸”æœ‰ç‚¹äº‘çš„æ£€æµ‹
            if (confidence >= self.confidence_threshold and 
                i < len(point_clouds) and 
                point_clouds[i]["point_cloud"] is not None):
                
                # ç›´æ¥æ›´æ–°è¯¥ç±»åˆ«çš„æ£€æµ‹ç»“æœ
                self.detected_objects[label] = [
                    label,
                    confidence,
                    point_clouds[i]["point_cloud"]
                ]
                
                self.get_logger().info(f"ğŸ”„ æ›´æ–°ç±»åˆ« '{label}': ç½®ä¿¡åº¦ {confidence:.3f}")
            else:
                self.get_logger().debug(f" è·³è¿‡ '{label}': ç½®ä¿¡åº¦ {confidence:.3f} < {self.confidence_threshold} æˆ–æ— ç‚¹äº‘")
        
    def _transform_point_cloud(self, point_cloud_numpy: np.ndarray, source_frame: str, target_frame: str, timestamp: rclpy.time.Time) -> Optional[np.ndarray]:
        """
        å°†ä¸€ä¸ªNumPyç‚¹äº‘ä»æºåæ ‡ç³»è½¬æ¢åˆ°ç›®æ ‡åæ ‡ç³»

        Args:
            point_cloud_numpy: (N, 3) çš„NumPyæ•°ç»„
            source_frame: æºåæ ‡ç³» (ä¾‹å¦‚ 'camera_color_optical_frame')
            target_frame: ç›®æ ‡åæ ‡ç³» (ä¾‹å¦‚ 'base_link')

        Returns:
            è½¬æ¢åçš„ (N, 3) NumPyæ•°ç»„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        if point_cloud_numpy.size == 0:
            return np.array([]) # å¦‚æœç‚¹äº‘ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºæ•°ç»„
        
        self.get_logger().info(f"è¯·æ±‚çš„æ—¶é—´æˆ³: {timestamp.sec}.{timestamp.nanosec}")
        self.get_logger().info(f"å½“å‰æ—¶é—´æˆ³: {self.get_clock().now().to_msg().sec}.{self.get_clock().now().to_msg().nanosec}")

        try:
            time_start = time.time()
            # 1. æŸ¥æ‰¾æŒ‡å®šæ—¶é—´æˆ³çš„å˜æ¢
            transform = self.tf_buffer.lookup_transform(
                target_frame,
                source_frame,
                rclpy.time.Time())
            time_end = time.time()
            self.get_logger().info(f"ğŸ”„ æŸ¥æ‰¾å˜æ¢è€—æ—¶: {time_end - time_start:.3f} ç§’")
            # transform = self.tf_buffer.lookup_transform(
            #     target_frame,
            #     source_frame,
            #     timestamp,  # <--- ä½¿ç”¨ä¼ å…¥çš„æ—¶é—´æˆ³
            #     timeout=rclpy.duration.Duration(seconds=0.1) # å¢åŠ ä¸€ä¸ªçŸ­æš‚è¶…æ—¶
            # )
            stamp_sec = transform.header.stamp.sec + transform.header.stamp.nanosec / 1e9
            now_sec = self.get_clock().now().nanoseconds / 1e9
            delay = now_sec - stamp_sec
            self.get_logger().info(f"TF stamp: {stamp_sec}, å½“å‰æ—¶é—´: {now_sec}, å»¶è¿Ÿ: {delay:.2f} ç§’")

            # 2. æå–å¹³ç§»å’Œæ—‹è½¬ (Scipyå¤„ç†)
            t = transform.transform.translation
            translation = np.array([t.x, t.y, t.z])

            q = transform.transform.rotation
            rotation = R.from_quat([q.x, q.y, q.z, q.w])

            # 3. çŸ©é˜µè¿ç®—åº”ç”¨å˜æ¢ (æ ¸å¿ƒåŠ é€Ÿéƒ¨åˆ†)
            time_start = time.time()
            # P_new = R * P_old + T
            transformed_points = rotation.apply(point_cloud_numpy) + translation
            
            time_end = time.time()
            self.get_logger().info(f"ğŸ”„ ç‚¹äº‘å˜æ¢è€—æ—¶(Vectorized): {time_end - time_start:.6f} ç§’")
            
            return transformed_points.astype(np.float32)

        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"åæ ‡å˜æ¢å¤±è´¥: ä» '{source_frame}' åˆ° '{target_frame}': {e}")
            return None

            # transformed_points = []
            # time_start_tf = time.time()
            # for point in point_cloud_numpy:
            #     # å°†NumPyç‚¹å°è£…æˆPointStampedæ¶ˆæ¯
            #     p_stamped = PointStamped()
            #     p_stamped.header.frame_id = source_frame
            #     p_stamped.point.x = float(point[0])
            #     p_stamped.point.y = float(point[1])
            #     p_stamped.point.z = float(point[2])

            #     # åº”ç”¨å˜æ¢
            #     p_transformed = tf2_geometry_msgs.do_transform_point(p_stamped, transform)
                
            #     transformed_points.append([
            #         p_transformed.point.x,
            #         p_transformed.point.y,
            #         p_transformed.point.z
            #     ])
            # time_end_tf = time.time()
            # self.get_logger().debug(f"ç‚¹äº‘å˜æ¢è€—æ—¶: {time_end_tf - time_start_tf:.6f} ç§’")
            
            # return np.array(transformed_points, dtype=np.float32)

        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            self.get_logger().error(f"åæ ‡å˜æ¢å¤±è´¥: ä» '{source_frame}' åˆ° '{target_frame}': {e}")
            return None

    def _create_point_cloud_msg(self, points: np.ndarray, colors: np.ndarray, frame_id: str) -> PointCloud2:
        """
        æ ¹æ®ç‚¹å’Œé¢œè‰²æ•°æ®åˆ›å»ºPointCloud2æ¶ˆæ¯
        """
        # 2. åˆ›å»ºä¸€ä¸ª Header å¯¹è±¡
        header = Header()
        header.stamp = self.get_clock().now().to_msg()
        header.frame_id = frame_id

        # å®šä¹‰ç‚¹äº‘å­—æ®µ
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1),
        ]
        
        # å°†é¢œè‰²(R,G,B)åˆå¹¶åˆ°ä¸€ä¸ªUINT32å­—æ®µä¸­
        colors_bgr = colors[:, [2, 1, 0]]
        rgb_packed = np.array((colors_bgr[:, 2] << 16) | (colors_bgr[:, 1] << 8) | (colors_bgr[:, 0]), dtype=np.uint32)
        
        # å°†ç‚¹å’Œé¢œè‰²æ•°æ®åˆå¹¶
        # åˆ›å»ºä¸€ä¸ªç»“æ„åŒ–æ•°ç»„
        point_data = np.zeros(points.shape[0], dtype=[
            ('x', np.float32),
            ('y', np.float32),
            ('z', np.float32),
            ('rgb', np.uint32)
        ])
        point_data['x'] = points[:, 0]
        point_data['y'] = points[:, 1]
        point_data['z'] = points[:, 2]
        point_data['rgb'] = rgb_packed

        # åˆ›å»ºPointCloud2æ¶ˆæ¯
        pc_msg = PointCloud2(
            header=header,
            height=1,
            width=points.shape[0],
            is_dense=True,
            is_bigendian=False,
            fields=fields,
            point_step=16, # 4 (x) + 4 (y) + 4 (z) + 4 (rgb)
            row_step=16 * points.shape[0],
            data=point_data.tobytes()
        )
        pc_msg.header.frame_id = frame_id
        
        return pc_msg

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.enable_image_visualization:
            cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ROS2èŠ‚ç‚¹"""
    rclpy.init()
    
    # å·¦æ‰‹ç›¸æœºå†…å‚
    camera_intrinsics = {
        "fx": 608.837158203125,  # å®é™…ç„¦è·x
        "fy": 609.1549682617188,  # å®é™…ç„¦è·y
        "cx": 424.99688720703125,  # å®é™…ä¸»ç‚¹x  
        "cy": 245.81431579589844   # å®é™…ä¸»ç‚¹y
    }
    # å³æ‰‹ç›¸æœºå†…å‚
    # camera_intrinsics = {
    #     "fx": 431.7814636230469,  # å®é™…ç„¦è·x
    #     "fy": 431.7814636230469,  # å®é™…ç„¦è·y
    #     "cx": 423.0641174316406,  # å®é™…ä¸»ç‚¹x  
    #     "cy": 235.52688598632812   # å®é™…ä¸»ç‚¹y
    # }
    
    # åˆ›å»ºYOLOæ£€æµ‹èŠ‚ç‚¹
    node = YOLOROS2Node(
        node_name="yolo_detector",
        model_path="non_ros_pkg/YOLO/weights/best.pt",  # YOLOæ¨¡å‹è·¯å¾„
        confidence_threshold=0.25,  # YOLOç½®ä¿¡åº¦é˜ˆå€¼
        imgsz=640,  # è¾“å…¥å›¾åƒå°ºå¯¸
        camera_intrinsics=camera_intrinsics,
        enable_image_visualization=False,  # è®¾ç½®ä¸ºTrueå¯å¼€å¯2Då›¾åƒæ£€æµ‹ç»“æœçª—å£
        enable_pointcloud_visualization=False, # è®¾ç½®ä¸ºTrueå¯å¼€å¯3Dç‚¹äº‘å¤„ç†çª—å£
        target_class_name="takeout bag"  # è®¾ç½®ä½ è®­ç»ƒçš„YOLOæ¨¡å‹ä¸­çš„ç›®æ ‡ç±»åˆ«åç§°
    )
    
    print("\n" + "="*60)
    print("ğŸ¯ YOLO + ROS2 æ£€æµ‹ç³»ç»Ÿ")
    print("="*60)
    print("ğŸ“¡ ROS2è¯é¢˜:")
    print("  è®¢é˜… RGB: /nbman/camera/nbman_head_rgbd/color/image_raw")
    print("  è®¢é˜… æ·±åº¦: /nbman/camera/nbman_head_rgbd/aligned_depth_to_color/image_raw")
    print("  å‘å¸ƒ æŠ“å–ä½å§¿: /grounding_dino/grasp_pose")
    print("="*60)
    print(f"â„¹ï¸  2Då›¾åƒå¯è§†åŒ–: {'å¯ç”¨' if node.enable_image_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  3Dç‚¹äº‘å¯è§†åŒ–: {'å¯ç”¨' if node.enable_pointcloud_visualization else 'ç¦ç”¨'}")
    print(f"â„¹ï¸  æŠ“å–ç›®æ ‡: '{node.target_label}'")
    print("="*60 + "\n")
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        node.get_logger().error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        node.cleanup()
        node.destroy_node()
        rclpy.shutdown()

if __name__ == "__main__":
    main()